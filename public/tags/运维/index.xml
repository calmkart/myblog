<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>运维 on cAlm的个人Blog</title>
        <link>http://localhost:1313/tags/%E8%BF%90%E7%BB%B4/</link>
        <description>Recent content in 运维 on cAlm的个人Blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <lastBuildDate>Wed, 25 Dec 2019 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/tags/%E8%BF%90%E7%BB%B4/index.xml" rel="self" type="application/rss+xml" /><item>
            <title>琐事繁多是不是一定不好?</title>
            <link>http://localhost:1313/posts/2019/12/2019-12-25-%E7%90%90%E4%BA%8B%E7%B9%81%E5%A4%9A%E6%98%AF%E4%B8%8D%E6%98%AF%E4%B8%80%E5%AE%9A%E4%B8%8D%E5%A5%BD/</link>
            <pubDate>Wed, 25 Dec 2019 00:00:00 +0000</pubDate>
            <guid>http://localhost:1313/posts/2019/12/2019-12-25-%E7%90%90%E4%BA%8B%E7%B9%81%E5%A4%9A%E6%98%AF%E4%B8%8D%E6%98%AF%E4%B8%80%E5%AE%9A%E4%B8%8D%E5%A5%BD/</guid>
            <description>&lt;p&gt;摘抄自 《SRE: Google运维解密》&lt;/p&gt;&#xA;&lt;p&gt;字字珠玑。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;琐事不会总是让每个人都不开心，特别是不太多的时候。已知的和重复性的工作有一种让人平静的功效。完成这些事可以带来一种满足感和快速胜利感。琐事可能是低风险低压力的活动，有些员工甚至喜欢做这种类型的工作。&lt;/p&gt;&#xA;&lt;p&gt;琐事的存在并不总是坏事，但是每个人都必须清楚，在SRE所扮演的角色中，一定数量的琐事是不可避免的，这其实是任何工程类工作都具有的特点。少量的琐事存在不是什么大问题。但是一旦琐事的数量变多，就会有害了。如果琐事特别繁重，那就应该非常担扰，大声抱怨。在许多琐事有害的原因中，有如下因素需要考虑：&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;职业停滞&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;如果花在工程项目上的时间太少，你的职业发展会变慢，甚至停滞。Google确实会奖励做那些脏活累活的人，但是仅仅是该工作是不可避免，并有巨大的正面影响的时候才会这样做。没有人可以通过不停地做脏活累活满足自己的职业发展。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;士气低落&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;每个人对自己可以承担的琐事限度有所不同，但是一定有个限度。过多的琐事会导致过度劳累，疲倦和不满。&lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p&gt;另外，牺牲工程实践而做琐事会对SRE组织的整体发展造成损害，原因如下：&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;造成误解&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;我们努力确保每个SRE以及与SRE一起工作的人都理解SRE是一个工程组织。如果个人或团队过度参与琐事，会破坏这种角色，造成误解。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;进展缓慢&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;琐事过多会导致团队生产力下降，如果SRE团队忙于为手工操作和导出数据救火，新功能的发布就会变慢。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;开创先例&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;如果SRE过于愿意承担琐事，研发同事就更倾向于加入更多的琐事，有时候甚至将本该由研发团队承担的运维工作转给SRE来承担。其他团队也会开始指望SRE接受这样的工作，这显然是不好的。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;促进摩擦产生&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;即使你个人对琐事没有怨言，你现在的或未来的队友可能会很不开心。如果团队中引入了太多的琐事，其实就是在鼓励团队里最好的工程师开始寻找其他地方提供的更有价值的工作。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;违反承诺&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;那些为了项目工程工作而入职的新员工，以及转入SRE的老员工会有被欺骗的感觉，这非常不利于公司的士气。&lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p&gt;如果我们都致力于每一周通过工程工作消除一点琐事，就可以持续性地整顿服务。我们就可以将更多的力量投入到扩大服务规模的工程工作上去，或者是进行下一代的服务的架构设计，又或者是建立一套跨SRE使用的工具链。让我们多创新，少干琐事吧！&lt;/p&gt;</description>
        </item><item>
            <title>配置和使用kube-prometheus</title>
            <link>http://localhost:1313/posts/2019/08/2019-08-22-%E9%85%8D%E7%BD%AE%E5%92%8C%E4%BD%BF%E7%94%A8kube-prometheus/</link>
            <pubDate>Thu, 22 Aug 2019 00:00:00 +0000</pubDate>
            <guid>http://localhost:1313/posts/2019/08/2019-08-22-%E9%85%8D%E7%BD%AE%E5%92%8C%E4%BD%BF%E7%94%A8kube-prometheus/</guid>
            <description>&lt;p&gt;我们在k8s集群中使用云原生的promethues通常需要用到coreos的&lt;a class=&#34;link&#34; href=&#34;https://github.com/coreos/prometheus-operator&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;&#xA;    &gt;prometheus-operater&lt;/a&gt;，它可以方便的帮助我们在k8s中部署和配置使用prometheus。但prometheus并不是开箱即用的，如果要做到开箱即用的监控全家桶，官方提供了两个选择，分别是&lt;a class=&#34;link&#34; href=&#34;https://github.com/helm/charts/tree/master/stable/prometheus-operator&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;&#xA;    &gt;prometheus-operater helm chart&lt;/a&gt;和&lt;a class=&#34;link&#34; href=&#34;https://github.com/coreos/kube-prometheus&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;&#xA;    &gt;kube-prometheus&lt;/a&gt;。这两者都可以为我们提供开箱即用的方式部署promethues+alertmanager+promethues-push-gateway(kube-promethueus不包含,需要单独部署)+grafana全家桶，同时包含&lt;a class=&#34;link&#34; href=&#34;https://github.com/kubernetes-monitoring/kubernetes-mixin&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;&#xA;    &gt;kubernetes-mixin&lt;/a&gt;的一整套报警规则和node-exporter，kube-state-metrics等一系列metrics exporter。区别在于helm chart由社区维护，而kube-promethues由coreos维护。这里我们将以kube-prometheus为例，简要说明配置和使用方式。&lt;/p&gt;&#xA;&lt;p&gt;首先是部署，还是非常简单的，我们先将kube-prometheus的仓库clone下来&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;git clone https://github.com/coreos/kube-prometheus.git&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后根据官方文档操作即可&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ kubectl create -f manifests/&#xA;&#xA;# It can take a few seconds for the above &amp;#39;create manifests&amp;#39; command to fully create the following resources, so verify the resources are ready before proceeding.&#xA;$ until kubectl get customresourcedefinitions servicemonitors.monitoring.coreos.com ; do date; sleep 1; echo &amp;#34;&amp;#34;; done&#xA;$ until kubectl get servicemonitors --all-namespaces ; do date; sleep 1; echo &amp;#34;&amp;#34;; done&#xA;&#xA;$ kubectl apply -f manifests/ # This command sometimes may need to be done twice (to workaround a race condition).&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这里将自动为我们部署prometheus，alertmanager和grafana。我们接下来可以通过port-forward也可以通过ingress将服务暴露出来&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Prometheus&#xA;&#xA;$ kubectl --namespace monitoring port-forward svc/prometheus-k8s 9090&#xA;&#xA;Then access via http://localhost:9090&#xA;&#xA;Grafana&#xA;&#xA;$ kubectl --namespace monitoring port-forward svc/grafana 3000&#xA;&#xA;Then access via http://localhost:3000 and use the default grafana user:password of admin:admin.&#xA;&#xA;Alert Manager&#xA;&#xA;$ kubectl --namespace monitoring port-forward svc/alertmanager-main 9093&#xA;&#xA;Then access via http://localhost:9093&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;或者编写ingress&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# ingress-monitor.yaml&#xA;apiVersion: extensions/v1beta1&#xA;kind: Ingress&#xA;metadata:&#xA;  name: monitoring-ingress&#xA;  annotations:&#xA;    nginx.ingress.kubernetes.io/ssl-redirect: &amp;#34;false&amp;#34;&#xA;  namespace: monitoring&#xA;spec:&#xA;  rules:&#xA;  - host: k8s-prometheus.calmkart.com&#xA;    http:&#xA;      paths:&#xA;      - path: /&#xA;        backend:&#xA;          serviceName: prometheus-k8s&#xA;          servicePort: 9090&#xA;  - host: k8s-grafana.calmkart.com&#xA;    http:&#xA;      paths:&#xA;      - path: /&#xA;        backend:&#xA;          serviceName: grafana&#xA;          servicePort: 3000&#xA;  - host: k8s-alertmanager.calmkart.com&#xA;    http:&#xA;      paths:&#xA;      - path: /&#xA;        backend:&#xA;          serviceName: alertmanager-main&#xA;          servicePort: 9093&#xA;&#xA;# kubectl apply -f ingress-monitor.yaml&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后我们就可以访问到prometheus,alertmanager和grafana的服务页面了 &lt;a class=&#34;link&#34; href=&#34;http://www.calmkart.com/wp-content/uploads/2019/08/prometheus.jpg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;&#xA;    &gt;&lt;img src=&#34;images/prometheus.jpg&#34; alt=&#34;&#34; /&gt;&#xA;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;http://www.calmkart.com/wp-content/uploads/2019/08/alert-manager.jpg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;&#xA;    &gt;&lt;img src=&#34;images/alert-manager.jpg&#34; alt=&#34;&#34; /&gt;&#xA;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;http://www.calmkart.com/wp-content/uploads/2019/08/grafana.jpg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;&#xA;    &gt;&lt;img src=&#34;images/grafana.jpg&#34; alt=&#34;&#34; /&gt;&#xA;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;这里prometheus已经集成了一些k8s相关的exporter和kubernetes-mixin的报警规则，我们可以从prometheus的status-&amp;gt;rules和status-&amp;gt;target中查看到。&lt;/p&gt;&#xA;&lt;p&gt;接下来，我们部署&lt;a class=&#34;link&#34; href=&#34;https://github.com/helm/charts/tree/master/stable/prometheus-pushgateway&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;&#xA;    &gt;push-gateway&lt;/a&gt;&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;#可以参考我这里NodePort的values参数,也可以自行设置&#xA;# values.yaml&#xA;&#xA;# Default values for prometheus-pushgateway.&#xA;# This is a YAML-formatted file.&#xA;# Declare variables to be passed into your templates.&#xA;image:&#xA;  repository: prom/pushgateway&#xA;  tag: v0.9.0&#xA;  pullPolicy: IfNotPresent&#xA;&#xA;service:&#xA;  type: NodePort&#xA;  port: 9091&#xA;  targetPort: 9091&#xA;&#xA;# Optional pod annotations&#xA;podAnnotations: {}&#xA;&#xA;# Optional pod labels&#xA;podLabels: {}&#xA;&#xA;# Optional service labels&#xA;serviceLabels: {}&#xA;&#xA;# Optional serviceAccount labels&#xA;serviceAccountLabels: {}&#xA;&#xA;# Optional additional arguments&#xA;extraArgs: []&#xA;&#xA;# Optional additional environment variables&#xA;extraVars: []&#xA;&#xA;resources: {}&#xA;  # We usually recommend not to specify default resources and to leave this as a conscious&#xA;  # choice for the user. This also increases chances charts run on environments with little&#xA;  # resources, such as Minikube. If you do want to specify resources, uncomment the following&#xA;  # lines, adjust them as necessary, and remove the curly braces after &amp;#39;resources:&amp;#39;.&#xA;  # limits:&#xA;  #   cpu: 200m&#xA;  #    memory: 50Mi&#xA;  # requests:&#xA;  #   cpu: 100m&#xA;  #   memory: 30Mi&#xA;&#xA;serviceAccount:&#xA;  # Specifies whether a ServiceAccount should be created&#xA;  create: true&#xA;  # The name of the ServiceAccount to use.&#xA;  # If not set and create is true, a name is generated using the fullname template&#xA;  name:&#xA;&#xA;## Configure ingress resource that allow you to access the&#xA;## pushgateway installation. Set up the URL&#xA;## ref: http://kubernetes.io/docs/user-guide/ingress/&#xA;##&#xA;ingress:&#xA;  ## Enable Ingress.&#xA;  ##&#xA;  enabled: false&#xA;&#xA;    ## Annotations.&#xA;    ##&#xA;    # annotations:&#xA;    #   kubernetes.io/ingress.class: nginx&#xA;    #   kubernetes.io/tls-acme: &amp;#39;true&amp;#39;&#xA;&#xA;    ## Hostnames.&#xA;    ## Must be provided if Ingress is enabled.&#xA;    ##&#xA;    # hosts:&#xA;    #   - pushgateway.domain.com&#xA;&#xA;    ## TLS configuration.&#xA;    ## Secrets must be manually created in the namespace.&#xA;    ##&#xA;    # tls:&#xA;    #   - secretName: pushgateway-tls&#xA;    #     hosts:&#xA;    #       - pushgateway.domain.com&#xA;&#xA;tolerations: {}&#xA;  # - effect: NoSchedule&#xA;  #   operator: Exists&#xA;&#xA;## Node labels for pushgateway pod assignment&#xA;## Ref: https://kubernetes.io/docs/user-guide/node-selection/&#xA;##&#xA;nodeSelector: {}&#xA;&#xA;replicaCount: 1&#xA;&#xA;## Affinity for pod assignment&#xA;## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity&#xA;affinity: {}&#xA;&#xA;# Enable this if you&amp;#39;re using https://github.com/coreos/prometheus-operator&#xA;serviceMonitor:&#xA;  enabled: true&#xA;  namespace: monitoring&#xA;  # fallback to the prometheus default unless specified&#xA;  # interval: 10s&#xA;  ## Defaults to what&amp;#39;s used if you follow CoreOS [Prometheus Install Instructions](https://github.com/helm/charts/tree/master/stable/prometheus-operator#tldr)&#xA;  ## [Prometheus Selector Label](https://github.com/helm/charts/tree/master/stable/prometheus-operator#prometheus-operator-1)&#xA;  ## [Kube Prometheus Selector Label](https://github.com/helm/charts/tree/master/stable/prometheus-operator#exporters)&#xA;  selector:&#xA;    prometheus: kube-prometheus&#xA;  # Retain the job and instance labels of the metrics pushed to the Pushgateway&#xA;  # [Scraping Pushgateway](https://github.com/prometheus/pushgateway#configure-the-pushgateway-as-a-target-to-scrape)&#xA;  honorLabels: true&#xA;&#xA;# The values to set in the PodDisruptionBudget spec (minAvailable/maxUnavailable)&#xA;# If not set then a PodDisruptionBudget will not be created&#xA;podDisruptionBudget:&#xA;&#xA;# helm install --name push-gateway -f values.yaml stable/prometheus-pushgateway&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后我们来试着接入内部和外部的prometheus监控target.&lt;/p&gt;&#xA;&lt;p&gt;1.实现接入内部的target 无论是外部或内部的target都需要一个metrics-server目标，对于内部target而言，一般是一个服务，比如服务calm-server 在prometheus-operater的使用方式中，有一个crd叫serviceMonitor，我们创建一个新的serviceMonitor就创建了一个prometheus的target 我们首先查看monitoring命名空间中已有的serviceMonitor(既prometheus target)&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[xxx@xxxxx]# kubectl get servicemonitors.monitoring.coreos.com -n monitoring&#xA;NAME                      AGE&#xA;alertmanager              23d&#xA;coredns                   23d&#xA;grafana                   23d&#xA;ingress-nginx             17d&#xA;kube-apiserver            23d&#xA;kube-controller-manager   23d&#xA;kube-scheduler            23d&#xA;kube-state-metrics        23d&#xA;kubelet                   23d&#xA;node-exporter             23d&#xA;prometheus                23d&#xA;prometheus-operator       23d&#xA;prometheus-pushgateway    19d&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我们创建一个新的serviceMonitor,将calm-server的/metrics作为target # calm-server-serviceMonitor.yaml&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apiVersion: monitoring.coreos.com/v1&#xA;kind: ServiceMonitor&#xA;metadata:&#xA;  name: calm-server-metrics&#xA;  labels:&#xA;    k8s-app: calm-server-metrics&#xA;  namespace: monitoring&#xA;spec:&#xA;  namespaceSelector:&#xA;    any: true&#xA;  selector:&#xA;    matchLabels:&#xA;      app: calm-server&#xA;  endpoints:&#xA;  - port: web&#xA;    interval: 10s&#xA;    honorLabels: true&#xA;&#xA;# kubectl apply -f calm-server-serviceMonitor.yaml&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这里有几个需要注意的tips:&lt;/p&gt;&#xA;&lt;p&gt;1.如果service和prometheus不再同一个命名空间，需要设置namespaceSelector，可以单独设置需要搜索的namespace，也可以像上面那样设置为全局搜索。&lt;/p&gt;&#xA;&lt;p&gt;2.其次，endpoints中设置的port会按照interval设置的时间定时去:/metrics拉取数据，所以对应的服务需要提供相应的metrics(官方有非常多的exporter提供使用，可以参考&lt;a class=&#34;link&#34; href=&#34;https://prometheus.io/docs/instrumenting/exporters/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;&#xA;    &gt;https://prometheus.io/docs/instrumenting/exporters/&lt;/a&gt;)&lt;/p&gt;&#xA;&lt;p&gt;我们接下来再查看所有serviceMonitor crd api对象，就会发现创建成功，同时prometheus的state-&amp;gt;target中也会出现对应的target，我们也可以在prometheus中查询到对应的数据了。&lt;/p&gt;&#xA;&lt;p&gt;2.实现接入外部target 实现了接入内部的target，那么，k8s集群外部的服务想要接入该怎么办呢？当然还是通过监控k8s集群内service的方式，不是service对应的是一个外部的endpoint对象。下面我们将以calm-server服务为例，说明如何通过k8s的endpoint外部对象接入外部target监控。 首先我们创建一个endpoint对象&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# calm-server-endpoint.yaml&#xA;apiVersion: v1&#xA;kind: Endpoints&#xA;metadata:&#xA;  name: calm-server-metrics&#xA;subsets:&#xA;  - addresses:&#xA;    - ip: x.x.x.x&#xA;    ports:&#xA;    - name: metrics&#xA;      port: xxxx&#xA;      protocol: TCP&#xA;# kubectl apply -f calm-server-endpoint.yaml&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我们将ip和port替换成我们的外部服务，apply后就创建了一个endpoint api对象，我们可以通过kubectl get endpoints查看&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[xxx@xxxxxxx]# kubectl get endpoints&#xA;NAME                                  ENDPOINTS                                                           AGE&#xA;calm-server-metrics                   10.41.13.17:6789                                                    19d&#xA;kubernetes                            10.1.33.159:6443                                                    92d&#xA;push-gateway-prometheus-pushgateway   10.240.224.15:9091                                                  19d&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这里需要注意的是,endpoint对象是不区分namespaces的 接着，我们创建一个service，service的选择器选择calm-server-metrics这个外部endpoint&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# calm-server-metrics-service.yaml&#xA;&#xA;apiVersion: v1&#xA;kind: Service&#xA;metadata:&#xA;  name: calm-server-metrics&#xA;  labels:&#xA;    app: calm-server-metrics&#xA;spec:&#xA;  type: ExternalName&#xA;  externalName: x.x.x.x&#xA;  clusterIP: &amp;#34;&amp;#34;&#xA;  ports:&#xA;  - name: metrics&#xA;    port: xxxx&#xA;    protocol: TCP&#xA;    targetPort: 6789&#xA;&#xA;# kubectl apply -f calm-server-metrics-service.yaml&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我们这个时候访问这个服务，就等于访问了外部的endpoint，既外部服务 我们就可以像上面创建内部服务的serviceMonitor一样，创建serviceMonitor api对象从而更新prometheus target列表了。&lt;/p&gt;&#xA;&lt;p&gt;接下来我们会有一个问题，如何修改这一系列全家桶的配置呢?比如prometheus的配置，grafana的配置。比如我们需要修改smtp报警配置该怎么办呢？如果是非云原生环境，我们可以直接修改配置文件即可，但在云原生环境中不一样。&lt;/p&gt;&#xA;&lt;p&gt;kube-prometheus官方文档推荐的方式是使用&lt;a class=&#34;link&#34; href=&#34;https://jsonnet.org/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;&#xA;    &gt;jsonnet&lt;/a&gt;对官方库做编译修改。那么如何直接通过修改yaml的方式修改配置文件呢？下面将分别介绍如何修改全家桶中的各个配置文件。&lt;/p&gt;&#xA;&lt;p&gt;1.首先是prometheus的alert rules，我们可以通过修改prometheus-rules.yaml文件修改。 2.其次是alertmanager的config,我们可以修改alertmanager-secret.yaml文件,注意，这是一个secret对象，内容经过了base64加密，我们应该先将内容解密再做修改，修改后再加密替换即可。 3.最后是grafana的配置修改，参考了grafana官方的docker image之后，我们可以先修改grafana-deployment.yaml文件，为其增加一个volume,配置如下&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apiVersion: apps/v1beta2&#xA;kind: Deployment&#xA;metadata:&#xA;  labels:&#xA;    app: grafana&#xA;  name: grafana&#xA;  namespace: monitoring&#xA;spec:&#xA;  replicas: 1&#xA;  selector:&#xA;    matchLabels:&#xA;      app: grafana&#xA;  template:&#xA;    metadata:&#xA;      labels:&#xA;        app: grafana&#xA;    spec:&#xA;      containers:&#xA;      - image: grafana/grafana:6.2.2&#xA;        name: grafana&#xA;        ports:&#xA;        - containerPort: 3000&#xA;          name: http&#xA;        readinessProbe:&#xA;          httpGet:&#xA;            path: /api/health&#xA;            port: http&#xA;        resources:&#xA;          limits:&#xA;            cpu: 200m&#xA;            memory: 200Mi&#xA;          requests:&#xA;            cpu: 100m&#xA;            memory: 100Mi&#xA;        volumeMounts:&#xA;        - mountPath: /var/lib/grafana&#xA;          name: grafana-storage&#xA;          readOnly: false&#xA;        - mountPath: /etc/grafana/provisioning/datasources&#xA;          name: grafana-datasources&#xA;          readOnly: false&#xA;        - mountPath: /etc/grafana/provisioning/dashboards&#xA;          name: grafana-dashboards&#xA;          readOnly: false&#xA;        - mountPath: /grafana-dashboard-definitions/0/apiserver&#xA;          name: grafana-dashboard-apiserver&#xA;          readOnly: false&#xA;        - mountPath: /grafana-dashboard-definitions/0/controller-manager&#xA;          name: grafana-dashboard-controller-manager&#xA;          readOnly: false&#xA;        - mountPath: /grafana-dashboard-definitions/0/k8s-cluster-rsrc-use&#xA;          name: grafana-dashboard-k8s-cluster-rsrc-use&#xA;          readOnly: false&#xA;        - mountPath: /grafana-dashboard-definitions/0/k8s-node-rsrc-use&#xA;          name: grafana-dashboard-k8s-node-rsrc-use&#xA;          readOnly: false&#xA;        - mountPath: /grafana-dashboard-definitions/0/k8s-resources-cluster&#xA;          name: grafana-dashboard-k8s-resources-cluster&#xA;          readOnly: false&#xA;        - mountPath: /grafana-dashboard-definitions/0/k8s-resources-namespace&#xA;          name: grafana-dashboard-k8s-resources-namespace&#xA;          readOnly: false&#xA;        - mountPath: /grafana-dashboard-definitions/0/k8s-resources-pod&#xA;          name: grafana-dashboard-k8s-resources-pod&#xA;          readOnly: false&#xA;        - mountPath: /grafana-dashboard-definitions/0/k8s-resources-workload&#xA;          name: grafana-dashboard-k8s-resources-workload&#xA;          readOnly: false&#xA;        - mountPath: /grafana-dashboard-definitions/0/k8s-resources-workloads-namespace&#xA;          name: grafana-dashboard-k8s-resources-workloads-namespace&#xA;          readOnly: false&#xA;        - mountPath: /grafana-dashboard-definitions/0/kubelet&#xA;          name: grafana-dashboard-kubelet&#xA;          readOnly: false&#xA;        - mountPath: /grafana-dashboard-definitions/0/nodes&#xA;          name: grafana-dashboard-nodes&#xA;          readOnly: false&#xA;        - mountPath: /grafana-dashboard-definitions/0/persistentvolumesusage&#xA;          name: grafana-dashboard-persistentvolumesusage&#xA;          readOnly: false&#xA;        - mountPath: /grafana-dashboard-definitions/0/pods&#xA;          name: grafana-dashboard-pods&#xA;          readOnly: false&#xA;        - mountPath: /grafana-dashboard-definitions/0/prometheus-remote-write&#xA;          name: grafana-dashboard-prometheus-remote-write&#xA;          readOnly: false&#xA;        - mountPath: /grafana-dashboard-definitions/0/prometheus&#xA;          name: grafana-dashboard-prometheus&#xA;          readOnly: false&#xA;        - mountPath: /grafana-dashboard-definitions/0/proxy&#xA;          name: grafana-dashboard-proxy&#xA;          readOnly: false&#xA;        - mountPath: /grafana-dashboard-definitions/0/scheduler&#xA;          name: grafana-dashboard-scheduler&#xA;          readOnly: false&#xA;        - mountPath: /grafana-dashboard-definitions/0/statefulset&#xA;          name: grafana-dashboard-statefulset&#xA;          readOnly: false&#xA;        - mountPath: /etc/grafana&#xA;          name: config-custom&#xA;      nodeSelector:&#xA;        beta.kubernetes.io/os: linux&#xA;      securityContext:&#xA;        runAsNonRoot: true&#xA;        runAsUser: 65534&#xA;      serviceAccountName: grafana&#xA;      volumes:&#xA;      - emptyDir: {}&#xA;        name: grafana-storage&#xA;      - name: grafana-datasources&#xA;        secret:&#xA;          secretName: grafana-datasources&#xA;      - configMap:&#xA;          name: grafana-dashboards&#xA;        name: grafana-dashboards&#xA;      - configMap:&#xA;          name: grafana-dashboard-apiserver&#xA;        name: grafana-dashboard-apiserver&#xA;      - configMap:&#xA;          name: grafana-dashboard-controller-manager&#xA;        name: grafana-dashboard-controller-manager&#xA;      - configMap:&#xA;          name: grafana-dashboard-k8s-cluster-rsrc-use&#xA;        name: grafana-dashboard-k8s-cluster-rsrc-use&#xA;      - configMap:&#xA;          name: grafana-dashboard-k8s-node-rsrc-use&#xA;        name: grafana-dashboard-k8s-node-rsrc-use&#xA;      - configMap:&#xA;          name: grafana-dashboard-k8s-resources-cluster&#xA;        name: grafana-dashboard-k8s-resources-cluster&#xA;      - configMap:&#xA;          name: grafana-dashboard-k8s-resources-namespace&#xA;        name: grafana-dashboard-k8s-resources-namespace&#xA;      - configMap:&#xA;          name: grafana-dashboard-k8s-resources-pod&#xA;        name: grafana-dashboard-k8s-resources-pod&#xA;      - configMap:&#xA;          name: grafana-dashboard-k8s-resources-workload&#xA;        name: grafana-dashboard-k8s-resources-workload&#xA;      - configMap:&#xA;          name: grafana-dashboard-k8s-resources-workloads-namespace&#xA;        name: grafana-dashboard-k8s-resources-workloads-namespace&#xA;      - configMap:&#xA;          name: grafana-dashboard-kubelet&#xA;        name: grafana-dashboard-kubelet&#xA;      - configMap:&#xA;          name: grafana-dashboard-nodes&#xA;        name: grafana-dashboard-nodes&#xA;      - configMap:&#xA;          name: grafana-dashboard-persistentvolumesusage&#xA;        name: grafana-dashboard-persistentvolumesusage&#xA;      - configMap:&#xA;          name: grafana-dashboard-pods&#xA;        name: grafana-dashboard-pods&#xA;      - configMap:&#xA;          name: grafana-dashboard-prometheus-remote-write&#xA;        name: grafana-dashboard-prometheus-remote-write&#xA;      - configMap:&#xA;          name: grafana-dashboard-prometheus&#xA;        name: grafana-dashboard-prometheus&#xA;      - configMap:&#xA;          name: grafana-dashboard-proxy&#xA;        name: grafana-dashboard-proxy&#xA;      - configMap:&#xA;          name: grafana-dashboard-scheduler&#xA;        name: grafana-dashboard-scheduler&#xA;      - configMap:&#xA;          name: grafana-dashboard-statefulset&#xA;        name: grafana-dashboard-statefulset&#xA;      - configMap:&#xA;          name: grafana-config&#xA;        name: config-custom&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;其实就是将一个叫grafana-config的configMap作为volumeMount到/etc/grafana下。 然后我们创建这个configMap&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# grafana-configmap.yaml&#xA;&#xA;apiVersion: v1&#xA;kind: ConfigMap&#xA;metadata:&#xA;  name: grafana-config&#xA;  namespace: monitoring&#xA;data:&#xA;  grafana.ini: |&#xA;    [smtp]&#xA;    enabled = true&#xA;    host = smtp.calmkart.com:465&#xA;    user = calmkart@calmkart.com&#xA;    password = xxxxxxxxxx&#xA;    skip_verify = false&#xA;    from_address = calmkart@calmkart.com&#xA;    from_name = grafana&#xA;&#xA;# kubectl apply -f grafana-configmap.yaml&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这样就可以替换掉默认的配置文件了。&lt;/p&gt;&#xA;&lt;p&gt;另外还有关于如何为grafana增加plugin等等话题，可以参考官方的相关资料。 就简单介绍到这里吧。&lt;/p&gt;</description>
        </item><item>
            <title>yum源创建和更新自己的包</title>
            <link>http://localhost:1313/posts/2018/07/2018-07-23-yum%E6%BA%90%E5%88%9B%E5%BB%BA%E5%92%8C%E6%9B%B4%E6%96%B0%E8%87%AA%E5%B7%B1%E7%9A%84%E5%8C%85/</link>
            <pubDate>Mon, 23 Jul 2018 00:00:00 +0000</pubDate>
            <guid>http://localhost:1313/posts/2018/07/2018-07-23-yum%E6%BA%90%E5%88%9B%E5%BB%BA%E5%92%8C%E6%9B%B4%E6%96%B0%E8%87%AA%E5%B7%B1%E7%9A%84%E5%8C%85/</guid>
            <description>&lt;p&gt;以tengine为例，记录添加yum自用打包过程.&lt;/p&gt;&#xA;&lt;p&gt;首先,下载tengine&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;wget https://github.com/alibaba/tengine/archive/tengine-2.2.2.tar.gz&#xA;tar xzvf tengine-2.2.2.tar.gz&#xA;cd tengine-tengine-2.2.2/&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后安装打包工具fpm&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;yum -y install ruby rubygems ruby-devel rpm-build&#xA;gem sources -a http://ruby.taobao.org/&#xA;gem install fpm&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;编译安装到其他文件夹下&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;./configure --prefix=/home/maintain/nginx --conf-path=/home/shared/nginx/conf/nginx.conf --pid-path=/home/shared/nginx/logs/nginx.pid --error-log-path=/home/shared/nginx/logs/error.log --with-http_ssl_module --with-http_sub_module --with-http_dav_module --with-http_flv_module --with-http_gzip_static_module --with-http_stub_status_module --with-http_v2_module --with-http_realip_module --add-module=ngx_cache_purge-2.3&#xA;make&#xA;mkdir /tmp/installdir&#xA;make install DESTDIR=/tmp/installdir/&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;打包成rpm包&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;cd /tmp/installdir/&#xA;fpm -s dir -t rpm -n tengine -v 2.2.0 --iteration 1.el6 -C /tmp/installdir/ -p /root&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;打出来的包会在/root文件夹下&lt;/p&gt;&#xA;&lt;p&gt;然后将rpm安装包上传到自建yum仓库,刷新仓库&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;createrepo --update {{ path }}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;客户端刷新centos yum缓存即可&lt;/p&gt;</description>
        </item><item>
            <title>部署sentry捕获项目错误信息</title>
            <link>http://localhost:1313/posts/2018/01/2018-01-25-%E9%83%A8%E7%BD%B2sentry%E6%8D%95%E8%8E%B7%E9%A1%B9%E7%9B%AE%E9%94%99%E8%AF%AF%E4%BF%A1%E6%81%AF/</link>
            <pubDate>Thu, 25 Jan 2018 00:00:00 +0000</pubDate>
            <guid>http://localhost:1313/posts/2018/01/2018-01-25-%E9%83%A8%E7%BD%B2sentry%E6%8D%95%E8%8E%B7%E9%A1%B9%E7%9B%AE%E9%94%99%E8%AF%AF%E4%BF%A1%E6%81%AF/</guid>
            <description>&lt;p&gt;sentry是一款专用于捕捉项目错误信息的分布式开源软件，可以轻松的捕捉到不同项目的错误信息并汇总，方便实时查看上下文排错以及报警等。 最终效果图如下： &lt;a class=&#34;link&#34; href=&#34;http://www.calmkart.com/wp-content/uploads/2018/01/sentry1.jpg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;&#xA;    &gt;&lt;img src=&#34;images/sentry1.jpg&#34; alt=&#34;&#34; /&gt;&#xA;&lt;/a&gt; &lt;a class=&#34;link&#34; href=&#34;http://www.calmkart.com/wp-content/uploads/2018/01/sentry2.jpg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;&#xA;    &gt;&lt;img src=&#34;images/sentry2.jpg&#34; alt=&#34;&#34; /&gt;&#xA;&lt;/a&gt; &lt;a class=&#34;link&#34; href=&#34;http://www.calmkart.com/wp-content/uploads/2018/01/sentry3.jpg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;&#xA;    &gt;&lt;img src=&#34;images/sentry3.jpg&#34; alt=&#34;&#34; /&gt;&#xA;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;首先，sentry和ELK的不同之处在于，虽然都基于LOG的追踪，但sentry主要关注错误的捕获和上下文重出现报警，而ELK主要关注日志的处理。&lt;/p&gt;&#xA;&lt;h4 id=&#34;1sentry的部署&#34;&gt;1.sentry的部署&#xA;&lt;/h4&gt;&lt;p&gt;官方文档 &lt;a class=&#34;link&#34; href=&#34;https://docs.sentry.io/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;&#xA;    &gt;https://docs.sentry.io/&lt;/a&gt; 大致来说，主要分为如下几个步骤：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;#安装postgresql&#xA;apt-get install postgresql&#xA;apt-get install postgresql-contrib&#xA;&#xA;#安装celery，redis&#xA;pip install celery&#xA;apt-get install redis-server&#xA;&#xA;#安装需要的包&#xA;apt-get install python-setuptools python-dev libxslt1-dev gcc libffi-dev libjpeg-dev libxml2-dev libxslt-dev libyaml-dev libpq-dev&#xA;&#xA;#安装sentry，推荐使用venv环境&#xA;pip install sentry&#xA;&#xA;#生成配置文件,也可制定配置文件路径&#xA;#sentry init /etc/sentry &#xA;sentry init&#xA;&#xA;#配置数据库&#xA;sudo -u postgres psql&#xA;create user admin with password &amp;#39;admin&amp;#39;;&#xA;create database sentry owner admin;&#xA;grant all privileges on database sentry to admin;&#xA;&#xA;#提升该用户superuser权限&#xA;alter role admin superuser;&#xA;&#xA;#修改sentry的相关配置，配置数据库相关设置，与上数据库设置相符合&#xA;vi /root/.sentry/sentry.conf.py&#xA;&#xA;#初始化数据库，可以制定配置文件路径也可默认&#xA;#SENTRY_CONF=/etc/sentry sentry upgrade&#xA;sentry upgrade&#xA;&#xA;#添加celery用root用户跑的环境变量&#xA;export C_FORCE_ROOT=&amp;#34;true&amp;#34;&#xA;&#xA;#最后创建用户&#xA;sentry createuser&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在upgrade过程中可能会出现错误，根据具体错误类型判断如何排错，然后删掉数据库后重建数据库，再upgrade即可，一般都是数据库权限问题。&lt;/p&gt;&#xA;&lt;h4 id=&#34;2sentry的使用&#34;&gt;2.sentry的使用&#xA;&lt;/h4&gt;&lt;p&gt;主要要跑3个任务，如下&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;#SENTRY_CONF=/etc/sentry sentry run web&#xA;sentry run web&#xA;#SENTRY_CONF=/etc/sentry sentry run worker&#xA;sentry run worker&#xA;#SENTRY_CONF=/etc/sentry sentry run cron&#xA;sentry run cron&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我在测试环境中是用gosuv来跑的，效果如图： &lt;a class=&#34;link&#34; href=&#34;http://www.calmkart.com/wp-content/uploads/2018/01/gosuv.jpg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;&#xA;    &gt;&lt;img src=&#34;images/gosuv.jpg&#34; alt=&#34;&#34; /&gt;&#xA;&lt;/a&gt; 其中要注意的是，如果任务直接用&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;sentry run web&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这样来跑的话，会找不到配置文件，所以需要配置成如下结构： &lt;a class=&#34;link&#34; href=&#34;http://www.calmkart.com/wp-content/uploads/2018/01/gosuv2.jpg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;&#xA;    &gt;&lt;img src=&#34;images/gosuv2.jpg&#34; alt=&#34;&#34; /&gt;&#xA;&lt;/a&gt; 同时，还跑了一个用于测试监控捕获错误的flask程序，其中用到了venv，所以gosuv也好，supervisor也好，都需要做成如下设置&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;#找到venv的虚拟环境，用虚拟环境的bin运行项目&#xA;./venv/bin/python runserver.py&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;3sentry-web的应用和配置&#34;&gt;3.sentry web的应用和配置&#xA;&lt;/h4&gt;&lt;p&gt;默认起的sentry web占用的是9000端口，也可以自己修改端口号。 登陆ip:9000，输入刚才设置的邮箱账号密码进入sentry，填一下相关设置 点击右上角new project,选择你需要监控错误的对象，这里选择flask，然后会出现 &lt;a class=&#34;link&#34; href=&#34;http://www.calmkart.com/wp-content/uploads/2018/01/sentry4.jpg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;&#xA;    &gt;&lt;img src=&#34;images/sentry4.jpg&#34; alt=&#34;&#34; /&gt;&#xA;&lt;/a&gt; 主要关注这个DSN key，在已有的项目可以在这里查看 &lt;a class=&#34;link&#34; href=&#34;http://www.calmkart.com/wp-content/uploads/2018/01/sentry5.jpg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;&#xA;    &gt;&lt;img src=&#34;images/sentry5.jpg&#34; alt=&#34;&#34; /&gt;&#xA;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;然后首先在客户端安装maven[flask]&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;pip install raven[flask]&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;最后在flask程序里添加如下&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;#__init__.py里设置app，故添加如下&#xA;from raven.contrib.flask import Sentry&#xA;sentry = Sentry(app, &#xA; dsn=&amp;#39;http://f1010071f12c47768dc6b5a4700d9c2e:5643252bcc70471eb5232a632f3ea133@10.32.80.118:9000//2&amp;#39;&#xA; )&#xA;&#xA;#views.py里添加如下&#xA;from flask_security import login_required, current_user&#xA;from hello import app&#xA;from hello import sentry&#xA;&#xA;@app.route(&amp;#39;/error&amp;#39;)&#xA;def error():&#xA;    try:&#xA;        1/0&#xA;    except ZeroDivisionError:&#xA;        sentry.captureException()&#xA;        return &amp;#39;error&amp;#39;&#xA;&#xA;@app.route(&amp;#39;/raise&amp;#39;)&#xA;def auto_raise():&#xA;    raise IndexError&#xA;&#xA;@app.route(&amp;#39;/log&amp;#39;)&#xA;def log():&#xA;    sentry.captureMessage(&amp;#39;hello, world!&amp;#39;)&#xA;    return &amp;#39;logging&amp;#39;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;类似如图结构： &lt;a class=&#34;link&#34; href=&#34;http://www.calmkart.com/wp-content/uploads/2018/01/flask.jpg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;&#xA;    &gt;&lt;img src=&#34;images/flask.jpg&#34; alt=&#34;&#34; /&gt;&#xA;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;然后就可以在sentry中即时捕捉到对应的错误和logging了&lt;/p&gt;</description>
        </item><item>
            <title>supervisor集中化管理web工具cesi</title>
            <link>http://localhost:1313/posts/2018/01/2018-01-11-supervisor%E9%9B%86%E4%B8%AD%E5%8C%96%E7%AE%A1%E7%90%86web%E5%B7%A5%E5%85%B7cesi/</link>
            <pubDate>Thu, 11 Jan 2018 00:00:00 +0000</pubDate>
            <guid>http://localhost:1313/posts/2018/01/2018-01-11-supervisor%E9%9B%86%E4%B8%AD%E5%8C%96%E7%AE%A1%E7%90%86web%E5%B7%A5%E5%85%B7cesi/</guid>
            <description>&lt;p&gt;linux进程管理器supervisor是会经常被用到的，但服务器多了之后，每个服务器的进程也不方便管理。同时，supervisor自带的web界面比较简陋，所以尝试了一下官网推荐的一些第三方开源软件，推荐一下这个cesi。 最终效果如下： &lt;a class=&#34;link&#34; href=&#34;http://www.calmkart.com/wp-content/uploads/2018/01/1.jpg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;&#xA;    &gt;&lt;img src=&#34;images/1.jpg&#34; alt=&#34;&#34; /&gt;&#xA;&lt;/a&gt; [&lt;img src=&#34;images/2.jpg&#34; alt=&#34;&#34; /&gt;&#xA;&lt;/p&gt;&#xA;&lt;p&gt;](&lt;a class=&#34;link&#34; href=&#34;http://www.calmkart.com/wp-content/uploads/2018/01/2.jpg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;&#xA;    &gt;http://www.calmkart.com/wp-content/uploads/2018/01/2.jpg&lt;/a&gt;)&lt;/p&gt;&#xA;&lt;p&gt;1.首先是关于supervisor 通过apt或pip安装都可以&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apt-get install supervisor&#xA;#pip install supervisor&#xA;echo_supervisord_conf &amp;gt; /etc/supervisor/supervisord.conf&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;关于supervisor配置文件&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[unix_http_server]&#xA;#这里配置是否用unix socket通信来让supervisor与supervisorctl做通信&#xA;&#xA;[inet_http_server]&#xA;#这里是用的http的方式做通信&#xA;&#xA;[supervisorctl]&#xA;#这里选择supervisorctl到底用以上两种中的哪种方式来与supervisor通信，选择一种即可，记得填写密码&#xA;&#xA;[program:pro_name]&#xA;stdout_logfile = {path}&#xA;redirect_stderr = true  ;让stderr也写入stdout中&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;常用操作&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;supervisorctl reload #重启加载supervisor配置文件&#xA;supervisorctl update #只增加新增的配置文件&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2.关于cesi&lt;/p&gt;&#xA;&lt;p&gt;项目地址：&lt;a class=&#34;link&#34; href=&#34;https://github.com/Gamegos/cesi&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;&#xA;    &gt;https://github.com/Gamegos/cesi&lt;/a&gt; 安装cesi&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apt-get install sqlite3 python python-flask&#xA;git clone https://github.com/Gamegos/cesi&#xA;cd cesi&#xA;sqlite3 ./userinfo.db &amp;lt; userinfo.sql&#xA;cp cesi.conf /etc/cesi.conf&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;配置cesi.conf,我的配置如下，一看就懂了&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[node:118]&#xA;username = ***&#xA;password = ***&#xA;host = 127.0.0.1&#xA;port = 9001&#xA;&#xA;[node:calmkart]&#xA;username = ***&#xA;password = ***&#xA;host = 45.76.71.69&#xA;port = 9001&#xA;&#xA;[node:121]&#xA;username = ***&#xA;password = ***&#xA;host = *.*.*.121&#xA;port = 9001&#xA;&#xA;[environment:my_env]&#xA;members = 118,calmkart,121&#xA;&#xA;[cesi]&#xA;database = /root/cesi/userinfo.db&#xA;activity_log = /var/log/cesi.log&#xA;host = 0.0.0.0&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;用supervisor运行cesi,配置文件如下&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[program:cesi]&#xA;directory = /root/cesi/cesi/&#xA;command = python web.py&#xA;autostart = true&#xA;startsecs = 5&#xA;autorestart = true&#xA;startretries = 3&#xA;user = root&#xA;redirect_stderr = true&#xA;stdout_logfile = /var/log/cesi1.log&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;开启任务&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;supervisorctl start cesi&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;默认账号密码：admin,admin 端口需要改的自己去web.py里面改 you get it&lt;/p&gt;&#xA;&lt;p&gt;本测试服地址： &lt;a class=&#34;link&#34; href=&#34;http://cesi.calmkart.com&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;&#xA;    &gt;http://cesi.calmkart.com&lt;/a&gt; 此外，其他常用的supervisor相关第三方工具还有 suponoff gosuv&lt;/p&gt;</description>
        </item><item>
            <title>修改salt源码实现jobs.list_jobs加密账号密码</title>
            <link>http://localhost:1313/posts/2017/12/2017-12-18-%E4%BF%AE%E6%94%B9salt%E6%BA%90%E7%A0%81%E5%AE%9E%E7%8E%B0jobs-list_jobs%E5%8A%A0%E5%AF%86%E8%B4%A6%E5%8F%B7%E5%AF%86%E7%A0%81/</link>
            <pubDate>Mon, 18 Dec 2017 00:00:00 +0000</pubDate>
            <guid>http://localhost:1313/posts/2017/12/2017-12-18-%E4%BF%AE%E6%94%B9salt%E6%BA%90%E7%A0%81%E5%AE%9E%E7%8E%B0jobs-list_jobs%E5%8A%A0%E5%AF%86%E8%B4%A6%E5%8F%B7%E5%AF%86%E7%A0%81/</guid>
            <description>&lt;p&gt;在salt master中为各个minion运行各种命令，当输入命令中显式带有账号密码时，可以在master端通过 salt.runners.jobs.last_run() salt.runners.jobs.list_job() salt.runners.jobs.list_jobs() salt.runners.jobs.list_jobs_filter() 等种种方法查看到账号密码，这样就会有安全风险。 所以，可以通过修改salt源码的方式实现对显示密码的记录加密。最终效果如下图： &lt;img src=&#34;images/zzxg.jpg&#34; alt=&#34;&#34; /&gt;&#xA;&lt;/p&gt;&#xA;&lt;p&gt;修改方法很简单，首先找到系统中的salt文件夹，本测试机在&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;/usr/lib/python2.7/dist-packages/salt/&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;修改./runners/jobs.py文件&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;#添加函数,这里主要是正则匹配，可以匹配更多的pass选项，实现更多的隐藏密码&#xA;import re&#xA;def encry_pass(target):&#xA;    result = re.sub(r&amp;#34;(?&amp;lt;=--password )\w+&amp;#34;, &amp;#34;*&amp;#34;, target)&#xA;    return result&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;并在list_job()函数，list_jobs()函数，list_jobs_filter()函数，print_job()函数末尾分别加入如下代码&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;#list_job()&#xA;ret = eval(encry_pass(str(ret)))   #增加此行&#xA;return ret&#xA;&#xA;#list_jobs()&#xA;mret = eval(encry_pass(str(mret)))    #增加此行&#xA;return mret&#xA;&#xA;#list_jobs_filter()&#xA;ret = eval(encry_pass(str(ret)))    #增加此行    &#xA;return ret&#xA;&#xA;#print_job()&#xA;ret = eval(encry_pass(str(ret)))    #增加此行&#xA;return ret&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;搞定&lt;/p&gt;&#xA;&lt;p&gt;一点小补充： 为什么要用&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;ret = eval(encry_pass(str(ret)))&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;因为ret是dict类型，得先把dict类型转换成str类型再写入encry_pass()正则匹配函数的参数里，然后encry_pass()正则匹配函数返回一个str对象，而ret需要的是dict对象，所以需要再用eval()转回来,也就是 ret(dict)&amp;mdash;-&amp;gt;str(ret)&amp;mdash;&amp;ndash;&amp;gt;ret(str)&amp;mdash;&amp;ndash;&amp;gt;result(str)&amp;mdash;&amp;ndash;&amp;gt;eval(result)&amp;mdash;&amp;ndash;&amp;gt;result(dict)&amp;mdash;&amp;ndash;&amp;gt;ret(dict)&lt;/p&gt;</description>
        </item><item>
            <title>NFS搭建及端口安全控制</title>
            <link>http://localhost:1313/posts/2017/12/2017-12-05-nfs%E6%90%AD%E5%BB%BA%E5%8F%8A%E7%AB%AF%E5%8F%A3%E5%AE%89%E5%85%A8%E6%8E%A7%E5%88%B6/</link>
            <pubDate>Tue, 05 Dec 2017 00:00:00 +0000</pubDate>
            <guid>http://localhost:1313/posts/2017/12/2017-12-05-nfs%E6%90%AD%E5%BB%BA%E5%8F%8A%E7%AB%AF%E5%8F%A3%E5%AE%89%E5%85%A8%E6%8E%A7%E5%88%B6/</guid>
            <description>&lt;p&gt;NFS服务用于文件分享和文件同步，功能就不描述了，架构是C/S的模式&lt;/p&gt;&#xA;&lt;p&gt;一.简单配置&lt;/p&gt;&#xA;&lt;p&gt;1.服务端&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apt install nfs-kernel-server&#xA;vi /etc/exports&#xA;########添加如下#######&#xA;/opt/nfs *(ro,sync,no_subtree_check)&#xA;#NFS共享路径 IP(只读，同步，不检查父目录)&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;service nfs-kernel-server restart &#xA;service rpcbind restart&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;测试：showmount -e&lt;/p&gt;&#xA;&lt;p&gt;2.客户端&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apt-get install nfs-common&#xA;mount 远程nfs_ip:/路径 /本地路径&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p&gt;二.NFS服务端口控制&lt;/p&gt;&#xA;&lt;p&gt;服务端配置：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# /etc/default/nfs-common&#xA; STATDOPTS=&amp;#34;--port 32765 --outgoing-port 32766&amp;#34;&#xA;&#xA; # /etc/default/nfs-kernel-server&#xA; RPCMOUNTDOPTS=&amp;#34;-p 32767&amp;#34;&#xA;&#xA; # /etc/default/quota&#xA; RPCRQUOTADOPTS=&amp;#34;-p 32769&amp;#34;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt; # /etc/sysctl.conf&#xA;fs.nfs.nfs_callback_tcpport = 32764&#xA;fs.nfs.nlm_tcpport = 32768&#xA;fs.nfs.nlm_udpport = 32768&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;sysctl --system&#xA;service nfs-kernel-server restart&#xA;service rpcbind restart&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;则可以将服务端端口限制为:&lt;/p&gt;&#xA;&lt;p&gt;2049,111,32764-32769 TCP/UDP&lt;/p&gt;</description>
        </item><item>
            <title>saltstack的安装和基本使用</title>
            <link>http://localhost:1313/posts/2017/11/2017-11-29-saltstack%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</link>
            <pubDate>Wed, 29 Nov 2017 00:00:00 +0000</pubDate>
            <guid>http://localhost:1313/posts/2017/11/2017-11-29-saltstack%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</guid>
            <description>&lt;p&gt;saltstack可用于批量管理集群,用的是c/s架构，master管理多个minion&lt;/p&gt;&#xA;&lt;p&gt;测试系统：debian8.7&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;一.安装master&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;1.添加apt key&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;wget -O - https://repo.saltstack.com/apt/debian/8/amd64/latest/SALTSTACK-GPG-KEY.pub | sudo apt-key add -&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2.将saltstack源添加进apt源&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;vi /etc/apt/sources.list&#xA;#添加saltstack源&#xA;deb http://repo.saltstack.com/apt/debian/8/amd64/latest jessie main&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3.更新apt&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apt-get update&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;4.安装master端相关组件&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apt-get install salt-api&#xA;apt-get install salt-cloud&#xA;apt-get install salt-master&#xA;apt-get install salt-minion&#xA;apt-get install salt-ssh&#xA;apt-get install salt-syndic&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;5.配置master&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;vi /etc/salt/master&#xA;####################常见配置如下####################&#xA;#指定master，冒号后有一个空格&#xA;user: root&#xA;#自动接收minion&#xA;auto_accept: True&#xA;&#xA;#-------以下为可选--------------&#xA;#自动接收minion&#xA;auto_accept: True&#xA;#salt的运行线程，开的线程越多一般处理的速度越快，但一般不要超过CPU的个数&#xA;worker_threads: 10&#xA;# master的管理端口&#xA;publish_port : 4505&#xA;# master跟minion的通讯端口，用于文件服务，认证，接受返回结果等&#xA;ret_port : 4506&#xA;# 如果这个master运行的salt-syndic连接到了一个更高层级的master,那么这个参数需要配置成连接到的这个高层级master的监听端口&#xA;syndic_master_port : 4506&#xA;# 指定pid文件位置&#xA;pidfile: /var/run/salt-master.pid&#xA;# saltstack 可以控制的文件系统的开始位置&#xA;root_dir: /&#xA;# 日志文件地址&#xA;log_file: /var/log/salt_master.log&#xA;# 分组设置&#xA;nodegroups:&#xA;  group_all: &amp;#39;*&amp;#39;&#xA;# salt state执行时候的根目录&#xA;file_roots:&#xA;  base:&#xA;    - /srv/salt/&#xA;# 设置pillar 的根目录&#xA;pillar_roots:&#xA;  base:&#xA;    - /srv/pillar&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;6.启动master&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;systemctl start salt-master&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;二.安装minion&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;1.安装salt-minion&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apt-get install salt-minion&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2.配置minion相关信息&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;vi /etc/salt/minion&#xA;&#xA;###################以下为常见设置###########################&#xA;#指定master，冒号后有一个空格&#xA;master: 10.32.80.121&#xA;&#xA;#-------以下为可选--------------&#xA;# minion的识别ID，可以是IP，域名，或是可以通过DNS解析的字符串&#xA;id: xx&#xA;# salt运行的用户权限&#xA;user: root&#xA;# master的识别ID，可以是IP，域名，或是可以通过DNS解析的字符串&#xA;master : ip&#xA;# master通讯端口&#xA;master_port: 4506&#xA;# 备份模式，minion是本地备份，当进行文件管理时的文件备份模式&#xA;backup_mode: minion&#xA;# 执行salt-call时候的输出方式&#xA;output: nested &#xA;# minion等待master接受认证的时间&#xA;acceptance_wait_time: 10&#xA;# 失败重连次数，0表示无限次，非零会不断尝试到设置值后停止尝试&#xA;acceptance_wait_time_max: 0&#xA;# 重新认证延迟时间，可以避免因为master的key改变导致minion需要重新认证的syn风暴&#xA;random_reauth_delay: 60&#xA;# 日志文件位置&#xA;log_file: /var/logs/salt_minion.log&#xA;# 文件路径基本位置&#xA;file_roots:&#xA;  base:&#xA;    - /etc/salt/minion/file&#xA;# pillar基本位置&#xA;pillar_roots:&#xA;  base:&#xA;    - /data/salt/minion/pillar&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3.启动minion&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;systemctl start salt-minion&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;三.通过master管理minion&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;1.配置key&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;#查看当前所有的key&#xA;salt-key&#xA;&#xA;#注册所有key&#xA;salt-key -A&#xA;&#xA;#注册某个key&#xA;salt-key -a minion_id&#xA;&#xA;#删除全部key&#xA;salt-key -D&#xA;&#xA;#删除某个key&#xA;salt-key -d minion_id&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2.测试是否连通&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;#测试所有minion的连通性&#xA;salt &amp;#39;*&amp;#39; test.ping&#xA;&#xA;#测试某个minion的连通性&#xA;salt &amp;#39;minion_id&amp;#39; test.ping&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3.常用命令&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;#salt cmd.run用于让minion批量执行命令&#xA;salt &amp;#39;*&amp;#39; cmd.run &amp;#39;ifconfig&amp;#39;&#xA;salt &amp;#39;minion_id&amp;#39; cmd.run &amp;#39;ifconfig&amp;#39;&#xA;&#xA;#salt-run manage查看minioni信息&#xA;salt-run manage.status #查看所有minion状态&#xA;salt-run manage.down ##查看down掉的minion&#xA;salt-run manage.up ##查看up的minion&#xA;&#xA;#salt-key管理密钥&#xA;salt-key [options]&#xA;salt-key -L              ##查看所有minion-key&#xA;salt-key -a &amp;lt;key-name&amp;gt;   ##接受某个minion-key&#xA;salt-key -d &amp;lt;key-name&amp;gt;   ##删除某个minion-key&#xA;salt-key -A              ##接受所有的minion-key&#xA;salt-key -D              ##删除所有的minion-key&#xA;&#xA;#salt-call在minion上执行某命令&#xA;salt-call [options] &amp;lt;function&amp;gt; [arguments]&#xA;salt-call test.ping           ##自己执行test.ping命令&#xA;salt-call cmd.run &amp;#39;ifconfig&amp;#39;  ##自己执行cmd.run函数&#xA;&#xA;#salt-cp分发文件给minion(不支持目录分发)&#xA;salt-cp &amp;#39;*&amp;#39; testfile.html /tmp   #把testfile.html发送到所有minion的/tmp文件夹&#xA;salt-cp &amp;#39;test*&amp;#39; index.html /tmp/a.html   #把index.html发送到所有test*的minion并重命名为/tmp/a.html&#xA;&#xA;#目标minion分组&#xA;修改/etc/salt/master&#xA;nodegroups:&#xA;  testgroup1: &amp;#39;L@test82.salt.cn,test83.salt.cn&amp;#39;&#xA;  testgroup2: &amp;#39;192.168.2.84&amp;#39;&#xA;#-N为分组命令符&#xA;salt -N testgroup1 test.ping&#xA;#分组分为多种类型：&#xA;    G -- 针对 Grains 做单个匹配，例如：G@os:Ubuntu&#xA;    E -- 针对 minion 针对正则表达式做匹配，例如：E@web\d+.(dev|qa|prod).loc&#xA;    P -- 针对 Grains 做正则表达式匹配，例如：P@os:(RedHat|Fedora|CentOS)&#xA;    L -- 针对 minion 做列表匹配，例如：L@minion1.example.com,minion3.domain.com or bl*.domain.com&#xA;    I -- 针对 Pillar 做单个匹配，例如：I@pdata:foobar&#xA;    S -- 针对子网或是 IP 做匹配，例如：S@192.168.1.0/24 or S@192.168.1.100&#xA;    R -- 针对客户端范围做匹配，例如： R@%foo.bar&#xA;&lt;/code&gt;&lt;/pre&gt;</description>
        </item></channel>
</rss>
