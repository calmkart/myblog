<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>云原生 on cAlm的个人Blog</title>
        <link>http://localhost:1313/tags/%E4%BA%91%E5%8E%9F%E7%94%9F/</link>
        <description>Recent content in 云原生 on cAlm的个人Blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <lastBuildDate>Mon, 21 Oct 2019 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/tags/%E4%BA%91%E5%8E%9F%E7%94%9F/index.xml" rel="self" type="application/rss+xml" /><item>
            <title>使用kubectl-debug来调试pod</title>
            <link>http://localhost:1313/posts/2019/10/2019-10-21-%E4%BD%BF%E7%94%A8kubectl-debug%E6%9D%A5%E8%B0%83%E8%AF%95pod/</link>
            <pubDate>Mon, 21 Oct 2019 00:00:00 +0000</pubDate>
            <guid>http://localhost:1313/posts/2019/10/2019-10-21-%E4%BD%BF%E7%94%A8kubectl-debug%E6%9D%A5%E8%B0%83%E8%AF%95pod/</guid>
            <description>&lt;p&gt;在k8s环境中,我们经常会碰到各种疑难杂症.比如下面这个例子: 某pod无法启动,查看日志显示原来是init时容器无法拉取某个外部网络上的包.我们exec登陆容器后试图调试下产生这个问题的原因,我们输入ping xxx.xxx.xxx,但sh直接提示&amp;quot;找不到ping命令&amp;quot;,甚至直接无法exec到一个没有sh的容器中. 这样的情况我们该怎么办呢？这里有一个解决类似问题的调试工具&lt;a class=&#34;link&#34; href=&#34;https://github.com/aylei/kubectl-debug&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;&#xA;    &gt;kubectl-debug&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;kubectl-debug的原理也很简单,因为docker容器是基于namespace做的隔离,所以可以创建一个新的预装好了各种调试工具的容器,直接加入待调试容器的namespace中,这样就可以与待调试pod共享各类栈,实现方便调试.(有点类似于k8s中的sidecar设计模式,但是使用的是一个用后既销的独立容器)&lt;/p&gt;&#xA;&lt;p&gt;这里介绍一下简单的使用方式 我们可以直接下载编译好的二进制文件包 &lt;a class=&#34;link&#34; href=&#34;https://github.com/aylei/kubectl-debug/releases&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;&#xA;    &gt;https://github.com/aylei/kubectl-debug/releases&lt;/a&gt; 也可以clone源代码自己做编译(可以获得更多最新修复和更新)&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# 这里要开启docker,因为是通过docker编译的&#xA;git clone https://github.com/aylei/kubectl-debug.git&#xA;cd kubectl-debug&#xA;make&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;最简单的使用方式&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl-debug POD_NAME&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;kubectl-debug将拉取nicolaka/netshoot镜像作为默认的debug镜像. 过一小会拉取镜像的功夫,我们创建好了一个插入待调试pod namespace中的调试容器,并且已经获取了stdin和stdout,我们就可以进行调试了. 调试完成后exit退出,kubectl-debug将完成相关的清理工作.&lt;/p&gt;&#xA;&lt;p&gt;当然,如果觉得默认的nicolaka/netshoot调试镜像不好用,也可以使用自己的私有镜像进行调试&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# 使用私有仓库镜像,并设置私有仓库使用的kubernetes secret&#xA;# secret data原文请设置为 {Username: , Password: }&#xA;# 默认secret_name为kubectl-debug-registry-secret,默认namspace为default&#xA;kubectl-debug POD_NAME --image calmkart/netshoot:latest --registry-secret-name  --registry-secret-namespace &#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;甚至如果原pod已经无法启动了,我们可以使用fork模式fork出一个新的待调试pod用于调试(自动替换掉entry-point)&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl debug POD_NAME --fork&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;更多功能可以参考&lt;a class=&#34;link&#34; href=&#34;https://github.com/aylei/kubectl-debug/blob/master/docs/zh-cn.md&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;&#xA;    &gt;kubectl-debug官方文档&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;总之这个工具还是非常实用的,我也参与添加了一些增强功能和bug修复.k8s官方据说也将在未来版本增加&lt;a class=&#34;link&#34; href=&#34;https://github.com/kubernetes/enhancements/issues/277&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;&#xA;    &gt;临时容器&lt;/a&gt;功能以支持调试,虽然不知道啥时候能用上,但还是期待的.&lt;/p&gt;</description>
        </item><item>
            <title>配置和使用kube-prometheus</title>
            <link>http://localhost:1313/posts/2019/08/2019-08-22-%E9%85%8D%E7%BD%AE%E5%92%8C%E4%BD%BF%E7%94%A8kube-prometheus/</link>
            <pubDate>Thu, 22 Aug 2019 00:00:00 +0000</pubDate>
            <guid>http://localhost:1313/posts/2019/08/2019-08-22-%E9%85%8D%E7%BD%AE%E5%92%8C%E4%BD%BF%E7%94%A8kube-prometheus/</guid>
            <description>&lt;p&gt;我们在k8s集群中使用云原生的promethues通常需要用到coreos的&lt;a class=&#34;link&#34; href=&#34;https://github.com/coreos/prometheus-operator&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;&#xA;    &gt;prometheus-operater&lt;/a&gt;，它可以方便的帮助我们在k8s中部署和配置使用prometheus。但prometheus并不是开箱即用的，如果要做到开箱即用的监控全家桶，官方提供了两个选择，分别是&lt;a class=&#34;link&#34; href=&#34;https://github.com/helm/charts/tree/master/stable/prometheus-operator&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;&#xA;    &gt;prometheus-operater helm chart&lt;/a&gt;和&lt;a class=&#34;link&#34; href=&#34;https://github.com/coreos/kube-prometheus&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;&#xA;    &gt;kube-prometheus&lt;/a&gt;。这两者都可以为我们提供开箱即用的方式部署promethues+alertmanager+promethues-push-gateway(kube-promethueus不包含,需要单独部署)+grafana全家桶，同时包含&lt;a class=&#34;link&#34; href=&#34;https://github.com/kubernetes-monitoring/kubernetes-mixin&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;&#xA;    &gt;kubernetes-mixin&lt;/a&gt;的一整套报警规则和node-exporter，kube-state-metrics等一系列metrics exporter。区别在于helm chart由社区维护，而kube-promethues由coreos维护。这里我们将以kube-prometheus为例，简要说明配置和使用方式。&lt;/p&gt;&#xA;&lt;p&gt;首先是部署，还是非常简单的，我们先将kube-prometheus的仓库clone下来&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;git clone https://github.com/coreos/kube-prometheus.git&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后根据官方文档操作即可&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ kubectl create -f manifests/&#xA;&#xA;# It can take a few seconds for the above &amp;#39;create manifests&amp;#39; command to fully create the following resources, so verify the resources are ready before proceeding.&#xA;$ until kubectl get customresourcedefinitions servicemonitors.monitoring.coreos.com ; do date; sleep 1; echo &amp;#34;&amp;#34;; done&#xA;$ until kubectl get servicemonitors --all-namespaces ; do date; sleep 1; echo &amp;#34;&amp;#34;; done&#xA;&#xA;$ kubectl apply -f manifests/ # This command sometimes may need to be done twice (to workaround a race condition).&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这里将自动为我们部署prometheus，alertmanager和grafana。我们接下来可以通过port-forward也可以通过ingress将服务暴露出来&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Prometheus&#xA;&#xA;$ kubectl --namespace monitoring port-forward svc/prometheus-k8s 9090&#xA;&#xA;Then access via http://localhost:9090&#xA;&#xA;Grafana&#xA;&#xA;$ kubectl --namespace monitoring port-forward svc/grafana 3000&#xA;&#xA;Then access via http://localhost:3000 and use the default grafana user:password of admin:admin.&#xA;&#xA;Alert Manager&#xA;&#xA;$ kubectl --namespace monitoring port-forward svc/alertmanager-main 9093&#xA;&#xA;Then access via http://localhost:9093&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;或者编写ingress&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# ingress-monitor.yaml&#xA;apiVersion: extensions/v1beta1&#xA;kind: Ingress&#xA;metadata:&#xA;  name: monitoring-ingress&#xA;  annotations:&#xA;    nginx.ingress.kubernetes.io/ssl-redirect: &amp;#34;false&amp;#34;&#xA;  namespace: monitoring&#xA;spec:&#xA;  rules:&#xA;  - host: k8s-prometheus.calmkart.com&#xA;    http:&#xA;      paths:&#xA;      - path: /&#xA;        backend:&#xA;          serviceName: prometheus-k8s&#xA;          servicePort: 9090&#xA;  - host: k8s-grafana.calmkart.com&#xA;    http:&#xA;      paths:&#xA;      - path: /&#xA;        backend:&#xA;          serviceName: grafana&#xA;          servicePort: 3000&#xA;  - host: k8s-alertmanager.calmkart.com&#xA;    http:&#xA;      paths:&#xA;      - path: /&#xA;        backend:&#xA;          serviceName: alertmanager-main&#xA;          servicePort: 9093&#xA;&#xA;# kubectl apply -f ingress-monitor.yaml&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后我们就可以访问到prometheus,alertmanager和grafana的服务页面了 &lt;a class=&#34;link&#34; href=&#34;http://www.calmkart.com/wp-content/uploads/2019/08/prometheus.jpg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;&#xA;    &gt;&lt;img src=&#34;images/prometheus.jpg&#34; alt=&#34;&#34; /&gt;&#xA;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;http://www.calmkart.com/wp-content/uploads/2019/08/alert-manager.jpg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;&#xA;    &gt;&lt;img src=&#34;images/alert-manager.jpg&#34; alt=&#34;&#34; /&gt;&#xA;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;http://www.calmkart.com/wp-content/uploads/2019/08/grafana.jpg&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;&#xA;    &gt;&lt;img src=&#34;images/grafana.jpg&#34; alt=&#34;&#34; /&gt;&#xA;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;这里prometheus已经集成了一些k8s相关的exporter和kubernetes-mixin的报警规则，我们可以从prometheus的status-&amp;gt;rules和status-&amp;gt;target中查看到。&lt;/p&gt;&#xA;&lt;p&gt;接下来，我们部署&lt;a class=&#34;link&#34; href=&#34;https://github.com/helm/charts/tree/master/stable/prometheus-pushgateway&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;&#xA;    &gt;push-gateway&lt;/a&gt;&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;#可以参考我这里NodePort的values参数,也可以自行设置&#xA;# values.yaml&#xA;&#xA;# Default values for prometheus-pushgateway.&#xA;# This is a YAML-formatted file.&#xA;# Declare variables to be passed into your templates.&#xA;image:&#xA;  repository: prom/pushgateway&#xA;  tag: v0.9.0&#xA;  pullPolicy: IfNotPresent&#xA;&#xA;service:&#xA;  type: NodePort&#xA;  port: 9091&#xA;  targetPort: 9091&#xA;&#xA;# Optional pod annotations&#xA;podAnnotations: {}&#xA;&#xA;# Optional pod labels&#xA;podLabels: {}&#xA;&#xA;# Optional service labels&#xA;serviceLabels: {}&#xA;&#xA;# Optional serviceAccount labels&#xA;serviceAccountLabels: {}&#xA;&#xA;# Optional additional arguments&#xA;extraArgs: []&#xA;&#xA;# Optional additional environment variables&#xA;extraVars: []&#xA;&#xA;resources: {}&#xA;  # We usually recommend not to specify default resources and to leave this as a conscious&#xA;  # choice for the user. This also increases chances charts run on environments with little&#xA;  # resources, such as Minikube. If you do want to specify resources, uncomment the following&#xA;  # lines, adjust them as necessary, and remove the curly braces after &amp;#39;resources:&amp;#39;.&#xA;  # limits:&#xA;  #   cpu: 200m&#xA;  #    memory: 50Mi&#xA;  # requests:&#xA;  #   cpu: 100m&#xA;  #   memory: 30Mi&#xA;&#xA;serviceAccount:&#xA;  # Specifies whether a ServiceAccount should be created&#xA;  create: true&#xA;  # The name of the ServiceAccount to use.&#xA;  # If not set and create is true, a name is generated using the fullname template&#xA;  name:&#xA;&#xA;## Configure ingress resource that allow you to access the&#xA;## pushgateway installation. Set up the URL&#xA;## ref: http://kubernetes.io/docs/user-guide/ingress/&#xA;##&#xA;ingress:&#xA;  ## Enable Ingress.&#xA;  ##&#xA;  enabled: false&#xA;&#xA;    ## Annotations.&#xA;    ##&#xA;    # annotations:&#xA;    #   kubernetes.io/ingress.class: nginx&#xA;    #   kubernetes.io/tls-acme: &amp;#39;true&amp;#39;&#xA;&#xA;    ## Hostnames.&#xA;    ## Must be provided if Ingress is enabled.&#xA;    ##&#xA;    # hosts:&#xA;    #   - pushgateway.domain.com&#xA;&#xA;    ## TLS configuration.&#xA;    ## Secrets must be manually created in the namespace.&#xA;    ##&#xA;    # tls:&#xA;    #   - secretName: pushgateway-tls&#xA;    #     hosts:&#xA;    #       - pushgateway.domain.com&#xA;&#xA;tolerations: {}&#xA;  # - effect: NoSchedule&#xA;  #   operator: Exists&#xA;&#xA;## Node labels for pushgateway pod assignment&#xA;## Ref: https://kubernetes.io/docs/user-guide/node-selection/&#xA;##&#xA;nodeSelector: {}&#xA;&#xA;replicaCount: 1&#xA;&#xA;## Affinity for pod assignment&#xA;## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity&#xA;affinity: {}&#xA;&#xA;# Enable this if you&amp;#39;re using https://github.com/coreos/prometheus-operator&#xA;serviceMonitor:&#xA;  enabled: true&#xA;  namespace: monitoring&#xA;  # fallback to the prometheus default unless specified&#xA;  # interval: 10s&#xA;  ## Defaults to what&amp;#39;s used if you follow CoreOS [Prometheus Install Instructions](https://github.com/helm/charts/tree/master/stable/prometheus-operator#tldr)&#xA;  ## [Prometheus Selector Label](https://github.com/helm/charts/tree/master/stable/prometheus-operator#prometheus-operator-1)&#xA;  ## [Kube Prometheus Selector Label](https://github.com/helm/charts/tree/master/stable/prometheus-operator#exporters)&#xA;  selector:&#xA;    prometheus: kube-prometheus&#xA;  # Retain the job and instance labels of the metrics pushed to the Pushgateway&#xA;  # [Scraping Pushgateway](https://github.com/prometheus/pushgateway#configure-the-pushgateway-as-a-target-to-scrape)&#xA;  honorLabels: true&#xA;&#xA;# The values to set in the PodDisruptionBudget spec (minAvailable/maxUnavailable)&#xA;# If not set then a PodDisruptionBudget will not be created&#xA;podDisruptionBudget:&#xA;&#xA;# helm install --name push-gateway -f values.yaml stable/prometheus-pushgateway&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后我们来试着接入内部和外部的prometheus监控target.&lt;/p&gt;&#xA;&lt;p&gt;1.实现接入内部的target 无论是外部或内部的target都需要一个metrics-server目标，对于内部target而言，一般是一个服务，比如服务calm-server 在prometheus-operater的使用方式中，有一个crd叫serviceMonitor，我们创建一个新的serviceMonitor就创建了一个prometheus的target 我们首先查看monitoring命名空间中已有的serviceMonitor(既prometheus target)&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[xxx@xxxxx]# kubectl get servicemonitors.monitoring.coreos.com -n monitoring&#xA;NAME                      AGE&#xA;alertmanager              23d&#xA;coredns                   23d&#xA;grafana                   23d&#xA;ingress-nginx             17d&#xA;kube-apiserver            23d&#xA;kube-controller-manager   23d&#xA;kube-scheduler            23d&#xA;kube-state-metrics        23d&#xA;kubelet                   23d&#xA;node-exporter             23d&#xA;prometheus                23d&#xA;prometheus-operator       23d&#xA;prometheus-pushgateway    19d&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我们创建一个新的serviceMonitor,将calm-server的/metrics作为target # calm-server-serviceMonitor.yaml&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apiVersion: monitoring.coreos.com/v1&#xA;kind: ServiceMonitor&#xA;metadata:&#xA;  name: calm-server-metrics&#xA;  labels:&#xA;    k8s-app: calm-server-metrics&#xA;  namespace: monitoring&#xA;spec:&#xA;  namespaceSelector:&#xA;    any: true&#xA;  selector:&#xA;    matchLabels:&#xA;      app: calm-server&#xA;  endpoints:&#xA;  - port: web&#xA;    interval: 10s&#xA;    honorLabels: true&#xA;&#xA;# kubectl apply -f calm-server-serviceMonitor.yaml&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这里有几个需要注意的tips:&lt;/p&gt;&#xA;&lt;p&gt;1.如果service和prometheus不再同一个命名空间，需要设置namespaceSelector，可以单独设置需要搜索的namespace，也可以像上面那样设置为全局搜索。&lt;/p&gt;&#xA;&lt;p&gt;2.其次，endpoints中设置的port会按照interval设置的时间定时去:/metrics拉取数据，所以对应的服务需要提供相应的metrics(官方有非常多的exporter提供使用，可以参考&lt;a class=&#34;link&#34; href=&#34;https://prometheus.io/docs/instrumenting/exporters/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;&#xA;    &gt;https://prometheus.io/docs/instrumenting/exporters/&lt;/a&gt;)&lt;/p&gt;&#xA;&lt;p&gt;我们接下来再查看所有serviceMonitor crd api对象，就会发现创建成功，同时prometheus的state-&amp;gt;target中也会出现对应的target，我们也可以在prometheus中查询到对应的数据了。&lt;/p&gt;&#xA;&lt;p&gt;2.实现接入外部target 实现了接入内部的target，那么，k8s集群外部的服务想要接入该怎么办呢？当然还是通过监控k8s集群内service的方式，不是service对应的是一个外部的endpoint对象。下面我们将以calm-server服务为例，说明如何通过k8s的endpoint外部对象接入外部target监控。 首先我们创建一个endpoint对象&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# calm-server-endpoint.yaml&#xA;apiVersion: v1&#xA;kind: Endpoints&#xA;metadata:&#xA;  name: calm-server-metrics&#xA;subsets:&#xA;  - addresses:&#xA;    - ip: x.x.x.x&#xA;    ports:&#xA;    - name: metrics&#xA;      port: xxxx&#xA;      protocol: TCP&#xA;# kubectl apply -f calm-server-endpoint.yaml&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我们将ip和port替换成我们的外部服务，apply后就创建了一个endpoint api对象，我们可以通过kubectl get endpoints查看&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[xxx@xxxxxxx]# kubectl get endpoints&#xA;NAME                                  ENDPOINTS                                                           AGE&#xA;calm-server-metrics                   10.41.13.17:6789                                                    19d&#xA;kubernetes                            10.1.33.159:6443                                                    92d&#xA;push-gateway-prometheus-pushgateway   10.240.224.15:9091                                                  19d&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这里需要注意的是,endpoint对象是不区分namespaces的 接着，我们创建一个service，service的选择器选择calm-server-metrics这个外部endpoint&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# calm-server-metrics-service.yaml&#xA;&#xA;apiVersion: v1&#xA;kind: Service&#xA;metadata:&#xA;  name: calm-server-metrics&#xA;  labels:&#xA;    app: calm-server-metrics&#xA;spec:&#xA;  type: ExternalName&#xA;  externalName: x.x.x.x&#xA;  clusterIP: &amp;#34;&amp;#34;&#xA;  ports:&#xA;  - name: metrics&#xA;    port: xxxx&#xA;    protocol: TCP&#xA;    targetPort: 6789&#xA;&#xA;# kubectl apply -f calm-server-metrics-service.yaml&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我们这个时候访问这个服务，就等于访问了外部的endpoint，既外部服务 我们就可以像上面创建内部服务的serviceMonitor一样，创建serviceMonitor api对象从而更新prometheus target列表了。&lt;/p&gt;&#xA;&lt;p&gt;接下来我们会有一个问题，如何修改这一系列全家桶的配置呢?比如prometheus的配置，grafana的配置。比如我们需要修改smtp报警配置该怎么办呢？如果是非云原生环境，我们可以直接修改配置文件即可，但在云原生环境中不一样。&lt;/p&gt;&#xA;&lt;p&gt;kube-prometheus官方文档推荐的方式是使用&lt;a class=&#34;link&#34; href=&#34;https://jsonnet.org/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;&#xA;    &gt;jsonnet&lt;/a&gt;对官方库做编译修改。那么如何直接通过修改yaml的方式修改配置文件呢？下面将分别介绍如何修改全家桶中的各个配置文件。&lt;/p&gt;&#xA;&lt;p&gt;1.首先是prometheus的alert rules，我们可以通过修改prometheus-rules.yaml文件修改。 2.其次是alertmanager的config,我们可以修改alertmanager-secret.yaml文件,注意，这是一个secret对象，内容经过了base64加密，我们应该先将内容解密再做修改，修改后再加密替换即可。 3.最后是grafana的配置修改，参考了grafana官方的docker image之后，我们可以先修改grafana-deployment.yaml文件，为其增加一个volume,配置如下&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apiVersion: apps/v1beta2&#xA;kind: Deployment&#xA;metadata:&#xA;  labels:&#xA;    app: grafana&#xA;  name: grafana&#xA;  namespace: monitoring&#xA;spec:&#xA;  replicas: 1&#xA;  selector:&#xA;    matchLabels:&#xA;      app: grafana&#xA;  template:&#xA;    metadata:&#xA;      labels:&#xA;        app: grafana&#xA;    spec:&#xA;      containers:&#xA;      - image: grafana/grafana:6.2.2&#xA;        name: grafana&#xA;        ports:&#xA;        - containerPort: 3000&#xA;          name: http&#xA;        readinessProbe:&#xA;          httpGet:&#xA;            path: /api/health&#xA;            port: http&#xA;        resources:&#xA;          limits:&#xA;            cpu: 200m&#xA;            memory: 200Mi&#xA;          requests:&#xA;            cpu: 100m&#xA;            memory: 100Mi&#xA;        volumeMounts:&#xA;        - mountPath: /var/lib/grafana&#xA;          name: grafana-storage&#xA;          readOnly: false&#xA;        - mountPath: /etc/grafana/provisioning/datasources&#xA;          name: grafana-datasources&#xA;          readOnly: false&#xA;        - mountPath: /etc/grafana/provisioning/dashboards&#xA;          name: grafana-dashboards&#xA;          readOnly: false&#xA;        - mountPath: /grafana-dashboard-definitions/0/apiserver&#xA;          name: grafana-dashboard-apiserver&#xA;          readOnly: false&#xA;        - mountPath: /grafana-dashboard-definitions/0/controller-manager&#xA;          name: grafana-dashboard-controller-manager&#xA;          readOnly: false&#xA;        - mountPath: /grafana-dashboard-definitions/0/k8s-cluster-rsrc-use&#xA;          name: grafana-dashboard-k8s-cluster-rsrc-use&#xA;          readOnly: false&#xA;        - mountPath: /grafana-dashboard-definitions/0/k8s-node-rsrc-use&#xA;          name: grafana-dashboard-k8s-node-rsrc-use&#xA;          readOnly: false&#xA;        - mountPath: /grafana-dashboard-definitions/0/k8s-resources-cluster&#xA;          name: grafana-dashboard-k8s-resources-cluster&#xA;          readOnly: false&#xA;        - mountPath: /grafana-dashboard-definitions/0/k8s-resources-namespace&#xA;          name: grafana-dashboard-k8s-resources-namespace&#xA;          readOnly: false&#xA;        - mountPath: /grafana-dashboard-definitions/0/k8s-resources-pod&#xA;          name: grafana-dashboard-k8s-resources-pod&#xA;          readOnly: false&#xA;        - mountPath: /grafana-dashboard-definitions/0/k8s-resources-workload&#xA;          name: grafana-dashboard-k8s-resources-workload&#xA;          readOnly: false&#xA;        - mountPath: /grafana-dashboard-definitions/0/k8s-resources-workloads-namespace&#xA;          name: grafana-dashboard-k8s-resources-workloads-namespace&#xA;          readOnly: false&#xA;        - mountPath: /grafana-dashboard-definitions/0/kubelet&#xA;          name: grafana-dashboard-kubelet&#xA;          readOnly: false&#xA;        - mountPath: /grafana-dashboard-definitions/0/nodes&#xA;          name: grafana-dashboard-nodes&#xA;          readOnly: false&#xA;        - mountPath: /grafana-dashboard-definitions/0/persistentvolumesusage&#xA;          name: grafana-dashboard-persistentvolumesusage&#xA;          readOnly: false&#xA;        - mountPath: /grafana-dashboard-definitions/0/pods&#xA;          name: grafana-dashboard-pods&#xA;          readOnly: false&#xA;        - mountPath: /grafana-dashboard-definitions/0/prometheus-remote-write&#xA;          name: grafana-dashboard-prometheus-remote-write&#xA;          readOnly: false&#xA;        - mountPath: /grafana-dashboard-definitions/0/prometheus&#xA;          name: grafana-dashboard-prometheus&#xA;          readOnly: false&#xA;        - mountPath: /grafana-dashboard-definitions/0/proxy&#xA;          name: grafana-dashboard-proxy&#xA;          readOnly: false&#xA;        - mountPath: /grafana-dashboard-definitions/0/scheduler&#xA;          name: grafana-dashboard-scheduler&#xA;          readOnly: false&#xA;        - mountPath: /grafana-dashboard-definitions/0/statefulset&#xA;          name: grafana-dashboard-statefulset&#xA;          readOnly: false&#xA;        - mountPath: /etc/grafana&#xA;          name: config-custom&#xA;      nodeSelector:&#xA;        beta.kubernetes.io/os: linux&#xA;      securityContext:&#xA;        runAsNonRoot: true&#xA;        runAsUser: 65534&#xA;      serviceAccountName: grafana&#xA;      volumes:&#xA;      - emptyDir: {}&#xA;        name: grafana-storage&#xA;      - name: grafana-datasources&#xA;        secret:&#xA;          secretName: grafana-datasources&#xA;      - configMap:&#xA;          name: grafana-dashboards&#xA;        name: grafana-dashboards&#xA;      - configMap:&#xA;          name: grafana-dashboard-apiserver&#xA;        name: grafana-dashboard-apiserver&#xA;      - configMap:&#xA;          name: grafana-dashboard-controller-manager&#xA;        name: grafana-dashboard-controller-manager&#xA;      - configMap:&#xA;          name: grafana-dashboard-k8s-cluster-rsrc-use&#xA;        name: grafana-dashboard-k8s-cluster-rsrc-use&#xA;      - configMap:&#xA;          name: grafana-dashboard-k8s-node-rsrc-use&#xA;        name: grafana-dashboard-k8s-node-rsrc-use&#xA;      - configMap:&#xA;          name: grafana-dashboard-k8s-resources-cluster&#xA;        name: grafana-dashboard-k8s-resources-cluster&#xA;      - configMap:&#xA;          name: grafana-dashboard-k8s-resources-namespace&#xA;        name: grafana-dashboard-k8s-resources-namespace&#xA;      - configMap:&#xA;          name: grafana-dashboard-k8s-resources-pod&#xA;        name: grafana-dashboard-k8s-resources-pod&#xA;      - configMap:&#xA;          name: grafana-dashboard-k8s-resources-workload&#xA;        name: grafana-dashboard-k8s-resources-workload&#xA;      - configMap:&#xA;          name: grafana-dashboard-k8s-resources-workloads-namespace&#xA;        name: grafana-dashboard-k8s-resources-workloads-namespace&#xA;      - configMap:&#xA;          name: grafana-dashboard-kubelet&#xA;        name: grafana-dashboard-kubelet&#xA;      - configMap:&#xA;          name: grafana-dashboard-nodes&#xA;        name: grafana-dashboard-nodes&#xA;      - configMap:&#xA;          name: grafana-dashboard-persistentvolumesusage&#xA;        name: grafana-dashboard-persistentvolumesusage&#xA;      - configMap:&#xA;          name: grafana-dashboard-pods&#xA;        name: grafana-dashboard-pods&#xA;      - configMap:&#xA;          name: grafana-dashboard-prometheus-remote-write&#xA;        name: grafana-dashboard-prometheus-remote-write&#xA;      - configMap:&#xA;          name: grafana-dashboard-prometheus&#xA;        name: grafana-dashboard-prometheus&#xA;      - configMap:&#xA;          name: grafana-dashboard-proxy&#xA;        name: grafana-dashboard-proxy&#xA;      - configMap:&#xA;          name: grafana-dashboard-scheduler&#xA;        name: grafana-dashboard-scheduler&#xA;      - configMap:&#xA;          name: grafana-dashboard-statefulset&#xA;        name: grafana-dashboard-statefulset&#xA;      - configMap:&#xA;          name: grafana-config&#xA;        name: config-custom&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;其实就是将一个叫grafana-config的configMap作为volumeMount到/etc/grafana下。 然后我们创建这个configMap&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# grafana-configmap.yaml&#xA;&#xA;apiVersion: v1&#xA;kind: ConfigMap&#xA;metadata:&#xA;  name: grafana-config&#xA;  namespace: monitoring&#xA;data:&#xA;  grafana.ini: |&#xA;    [smtp]&#xA;    enabled = true&#xA;    host = smtp.calmkart.com:465&#xA;    user = calmkart@calmkart.com&#xA;    password = xxxxxxxxxx&#xA;    skip_verify = false&#xA;    from_address = calmkart@calmkart.com&#xA;    from_name = grafana&#xA;&#xA;# kubectl apply -f grafana-configmap.yaml&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这样就可以替换掉默认的配置文件了。&lt;/p&gt;&#xA;&lt;p&gt;另外还有关于如何为grafana增加plugin等等话题，可以参考官方的相关资料。 就简单介绍到这里吧。&lt;/p&gt;</description>
        </item></channel>
</rss>
