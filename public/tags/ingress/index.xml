<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Ingress on cAlm的个人Blog</title>
        <link>http://localhost:1313/tags/ingress/</link>
        <description>Recent content in Ingress on cAlm的个人Blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <lastBuildDate>Sat, 25 May 2019 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/tags/ingress/index.xml" rel="self" type="application/rss+xml" /><item>
            <title>kubernetes集群中服务的负载均衡和外部发现</title>
            <link>http://localhost:1313/posts/2019/05/2019-05-25-kubernetes%E9%9B%86%E7%BE%A4%E4%B8%AD%E6%9C%8D%E5%8A%A1%E7%9A%84%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%92%8C%E5%A4%96%E9%83%A8%E5%8F%91%E7%8E%B0/</link>
            <pubDate>Sat, 25 May 2019 00:00:00 +0000</pubDate>
            <guid>http://localhost:1313/posts/2019/05/2019-05-25-kubernetes%E9%9B%86%E7%BE%A4%E4%B8%AD%E6%9C%8D%E5%8A%A1%E7%9A%84%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%92%8C%E5%A4%96%E9%83%A8%E5%8F%91%E7%8E%B0/</guid>
            <description>&lt;p&gt;传统的负载均衡策略一般是:&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;客户端 -&amp;gt; dns -&amp;gt; 4层库在均衡(HA) -&amp;gt; 7层负载均衡 -&amp;gt; 具体的后端服务&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;这种方式处理服务发现和动态配置比较难搞,比如后端服务动态扩缩容什么的。 一开始的做法肯定是OP人工维护7层负载均衡集群配置，以nginx为例，用git仓库之类的做人工服务发现和变更。这样很显然是不科学的。然后就会开始希望用种种自动化的手段去处理服务发现，比如微博的nginx-upsync-module,比如nginx-lua，比如openresty，比如etcd+confd等种种手段实现服务发现和自动化配置，但还是有很多瑕疵，比如etcd+confd每次修改upstream的服务后端就要reload nginx，nginx的策略会新开x个worker，容易造成性能问题甚至机器顶不住。而且vm服务机器的逻辑和upstream挂钩也有问题，总之，麻烦的很。&lt;/p&gt;&#xA;&lt;p&gt;而kubernetes处理这一套服务发现和负载均衡的方法就挺好用的，比如接下来会实践的这一套nginx-ingress+externalIP+dns的方式就很直观而且方便自动化流程，大致策略如下:&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;客户端 -&amp;gt; dns -&amp;gt; k8s-nginx-ingress-service(in ExternalIP) -&amp;gt; ingress-obj -&amp;gt; service -&amp;gt; pod&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;kubernetes集群中具体工作的pod是由service做负载均衡和服务发现的，而service的外部发现策略通常有NodePort，Kube-proxy以及Ingress。对于内部服务暴露给外部，NodePort基本上是不好用的，因为本来Port就有限，而且如果是web服务(80,443)，需要接dns的，开在30000+端口以上，还得在外部做一个四层负载均衡，这样就很烦。Kube-proxy用在调试环境还行，同样有以上问题，所以Ingress是更好的选择。&lt;/p&gt;&#xA;&lt;p&gt;nginx-ingress采用了nginx-lua模块实现upstream动态修正，无需reload nginx，可以解决上述提到的2x worker导致性能下降问题。且结合了service的pods自动发现(coredns)，轻松简单的完成很多过去很麻烦的任务。&lt;/p&gt;&#xA;&lt;p&gt;首先在上篇blog里我们已经有了一个k8s集群，然后我们创建ingress-nginx相关的api对象&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml&#xA;kubectl apply -f mandatory.yaml&#xA;&#xA;#会创建好相应的ingress-nginx controller和rbac相关api&#xA;[root@xxxxxxxx ingress-nginx]# kubectl get pods -n ingress-nginx&#xA;NAME                                        READY   STATUS    RESTARTS   AGE&#xA;nginx-ingress-controller-5694ccb578-hwj2j   1/1     Running   0          3h11m&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后我们创建一个ingress的service，用ExternalIP的方式暴露给集群外部&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;vim externalIp-ingress-service.yaml&#xA;&#xA;apiVersion: v1&#xA;kind: Service&#xA;metadata:&#xA;  name: ingress-nginx&#xA;  namespace: ingress-nginx&#xA;  labels:&#xA;    app.kubernetes.io/name: ingress-nginx&#xA;    app.kubernetes.io/part-of: ingress-nginx&#xA;spec:&#xA;  ports:&#xA;    - name: http&#xA;      port: 80&#xA;      protocol: TCP&#xA;    - name: https&#xA;      port: 443&#xA;      protocol: TCP&#xA;  selector:&#xA;    app.kubernetes.io/name: ingress-nginx&#xA;    app.kubernetes.io/part-of: ingress-nginx&#xA;  externalIPs:&#xA;    #这里填写你的k8s masterip&#xA;    - 10.1.33.159&#xA;&#xA;#将创建出如下服务&#xA;[root@xxxxxxxxxxxx ingress-nginx]# kubectl get service -n ingress-nginx&#xA;NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE&#xA;ingress-nginx   ClusterIP   10.96.139.222   10.1.33.159   80/TCP,443/TCP   3h12m&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;接下来我们就可以创建具体的ingress对象了&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;vim rook-ceph-ingress.yaml&#xA;&#xA;apiVersion: extensions/v1beta1&#xA;kind: Ingress&#xA;metadata:&#xA;  name: rook-ceph-ingress&#xA;  annotations:&#xA;    nginx.ingress.kubernetes.io/ssl-redirect: &amp;#34;false&amp;#34;&#xA;  #要暴露的service对应的命名空间  &#xA;  namespace: rook-ceph&#xA;spec:&#xA;  rules:&#xA;  #这里按需配置,就是nginx的配置方法,具体查下ingress-nginx的项目文档&#xA;  - host: k8s-ceph-dashboard.calmkart.com&#xA;    http:&#xA;      paths:&#xA;      - path: /&#xA;        backend:&#xA;          serviceName: rook-ceph-mgr-dashboard&#xA;          servicePort: 8443&#xA;&#xA;kubectl apply -f rook-ceph-ingress.yaml&#xA;#这个ingress将自动发现rook-ceph命名空间中的rook-ceph-mgr-dashboard服务，并将之作负载均衡对外暴露&#xA;[root@xxxxxxxxxxx ingress]# kubectl get service -n rook-ceph&#xA;NAME                      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE&#xA;rook-ceph-mgr             ClusterIP   10.99.37.148             9283/TCP            3h48m&#xA;rook-ceph-mgr-dashboard   ClusterIP   10.110.163.110           8443/TCP            3h48m&#xA;rook-ceph-mon-a           ClusterIP   10.103.5.55              6789/TCP,3300/TCP   3h49m&#xA;rook-ceph-mon-b           ClusterIP   10.104.22.199            6789/TCP,3300/TCP   3h49m&#xA;rook-ceph-mon-c           ClusterIP   10.110.80.82             6789/TCP,3300/TCP   3h49m&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后我们查看一下ingress状态&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[root@xxxxxxxxxx ingress]# kubectl get ingress -n rook-ceph&#xA;NAME                HOSTS                                ADDRESS       PORTS   AGE&#xA;rook-ceph-ingress   k8s-ceph-dashboard.calmkart.com   10.1.33.159   80      170m&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这样就没问题了 最后我们把dns记录k8s-ceph-dashboard.calmkart.com指向10.1.33.159&lt;/p&gt;&#xA;&lt;p&gt;搞定。&lt;/p&gt;</description>
        </item></channel>
</rss>
