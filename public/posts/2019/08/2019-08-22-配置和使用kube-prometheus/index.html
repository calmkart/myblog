<!DOCTYPE html>
<html lang="zh-cn" dir="ltr">
    <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content="我们在k8s集群中使用云原生的promethues通常需要用到coreos的prometheus-operater，它可以方便的帮助我们在k8s中部署和配置使用prometheus。但prometheus并不是开箱即用的，如果要做到开箱即用的监控全家桶，官方提供了两个选择，分别是prometheus-operater helm chart和kube-prometheus。这两者都可以为我们提供开箱即用的方式部署promethues+alertmanager+promethues-push-gateway(kube-promethueus不包含,需要单独部署)+grafana全家桶，同时包含kubernetes-mixin的一整套报警规则和node-exporter，kube-state-metrics等一系列metrics exporter。区别在于helm chart由社区维护，而kube-promethues由coreos维护。这里我们将以kube-prometheus为例，简要说明配置和使用方式。\n">
<title>配置和使用kube-prometheus</title>


<link rel="canonical" href="http://localhost:1313/posts/2019/08/2019-08-22-%E9%85%8D%E7%BD%AE%E5%92%8C%E4%BD%BF%E7%94%A8kube-prometheus/">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap">
<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap">


<link rel="stylesheet" href="/scss/style.min.390cc81d180d80110703ea467e355a4f3a0f44f25d4e957842790824fb5f2ef2.css"><meta property='og:title' content="配置和使用kube-prometheus">
<meta property='og:description' content="我们在k8s集群中使用云原生的promethues通常需要用到coreos的prometheus-operater，它可以方便的帮助我们在k8s中部署和配置使用prometheus。但prometheus并不是开箱即用的，如果要做到开箱即用的监控全家桶，官方提供了两个选择，分别是prometheus-operater helm chart和kube-prometheus。这两者都可以为我们提供开箱即用的方式部署promethues+alertmanager+promethues-push-gateway(kube-promethueus不包含,需要单独部署)+grafana全家桶，同时包含kubernetes-mixin的一整套报警规则和node-exporter，kube-state-metrics等一系列metrics exporter。区别在于helm chart由社区维护，而kube-promethues由coreos维护。这里我们将以kube-prometheus为例，简要说明配置和使用方式。\n">
<meta property='og:url' content='http://localhost:1313/posts/2019/08/2019-08-22-%E9%85%8D%E7%BD%AE%E5%92%8C%E4%BD%BF%E7%94%A8kube-prometheus/'>
<meta property='og:site_name' content='cAlm的个人Blog'>
<meta property='og:type' content='article'><meta property='article:section' content='Posts' /><meta property='article:tag' content='alertmanager' /><meta property='article:tag' content='grafana' /><meta property='article:tag' content='k8s' /><meta property='article:tag' content='kubernetes' /><meta property='article:tag' content='prometheus' /><meta property='article:tag' content='云原生' /><meta property='article:tag' content='容器' /><meta property='article:tag' content='监控' /><meta property='article:tag' content='运维' /><meta property='article:published_time' content='2019-08-22T00:00:00&#43;00:00'/><meta property='article:modified_time' content='2019-08-22T00:00:00&#43;00:00'/>
<meta name="twitter:title" content="配置和使用kube-prometheus">
<meta name="twitter:description" content="我们在k8s集群中使用云原生的promethues通常需要用到coreos的prometheus-operater，它可以方便的帮助我们在k8s中部署和配置使用prometheus。但prometheus并不是开箱即用的，如果要做到开箱即用的监控全家桶，官方提供了两个选择，分别是prometheus-operater helm chart和kube-prometheus。这两者都可以为我们提供开箱即用的方式部署promethues+alertmanager+promethues-push-gateway(kube-promethueus不包含,需要单独部署)+grafana全家桶，同时包含kubernetes-mixin的一整套报警规则和node-exporter，kube-state-metrics等一系列metrics exporter。区别在于helm chart由社区维护，而kube-promethues由coreos维护。这里我们将以kube-prometheus为例，简要说明配置和使用方式。\n">
    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "auto");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column compact"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="Toggle Menu">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
        
        <div class="site-meta">
            <h1 class="site-name"><a href="/">cAlm的个人Blog</a></h1>
            <h2 class="site-description">随手记录些东西</h2>
        </div>
    </header><ol class="menu" id="main-menu">
        
        
        
        <li >
            <a href='/' >
                
                <span>首页</span>
            </a>
        </li>
        
        
        <li >
            <a href='/archives' >
                
                <span>归档</span>
            </a>
        </li>
        
        
        <li >
            <a href='/categories' >
                
                <span>分类</span>
            </a>
        </li>
        
        
        <li >
            <a href='/about' >
                
                <span>关于</span>
            </a>
        </li>
        
        <li class="menu-bottom-section">
            <ol class="menu">

                
                    <li id="dark-mode-toggle">
                        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="8" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="16" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                        <span>Dark Mode</span>
                    </li>
                
            </ol>
        </li>
    </ol>
</aside>

    

            <main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    
    <header class="article-category">
        
            
            
            
            <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/" style="background-color: hsl(208, 60%, 84%); color: hsl(208, 60%, 15%);">
                计算机
            </a>
        
    </header>
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/posts/2019/08/2019-08-22-%E9%85%8D%E7%BD%AE%E5%92%8C%E4%BD%BF%E7%94%A8kube-prometheus/">配置和使用kube-prometheus</a>
        </h2>
    
        
    </div>

    
    
    
    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published" datetime='2019-08-22T00:00:00Z'>Thursday, August 22, 2019</time>
            </div>
        

        
    </footer>
    

    

    
</div>

</header>

    

    <section class="article-content">
    
    
    <p>我们在k8s集群中使用云原生的promethues通常需要用到coreos的<a class="link" href="https://github.com/coreos/prometheus-operator"  target="_blank" rel="noopener"
    >prometheus-operater</a>，它可以方便的帮助我们在k8s中部署和配置使用prometheus。但prometheus并不是开箱即用的，如果要做到开箱即用的监控全家桶，官方提供了两个选择，分别是<a class="link" href="https://github.com/helm/charts/tree/master/stable/prometheus-operator"  target="_blank" rel="noopener"
    >prometheus-operater helm chart</a>和<a class="link" href="https://github.com/coreos/kube-prometheus"  target="_blank" rel="noopener"
    >kube-prometheus</a>。这两者都可以为我们提供开箱即用的方式部署promethues+alertmanager+promethues-push-gateway(kube-promethueus不包含,需要单独部署)+grafana全家桶，同时包含<a class="link" href="https://github.com/kubernetes-monitoring/kubernetes-mixin"  target="_blank" rel="noopener"
    >kubernetes-mixin</a>的一整套报警规则和node-exporter，kube-state-metrics等一系列metrics exporter。区别在于helm chart由社区维护，而kube-promethues由coreos维护。这里我们将以kube-prometheus为例，简要说明配置和使用方式。</p>
<p>首先是部署，还是非常简单的，我们先将kube-prometheus的仓库clone下来</p>
<pre tabindex="0"><code>git clone https://github.com/coreos/kube-prometheus.git
</code></pre><p>然后根据官方文档操作即可</p>
<pre tabindex="0"><code>$ kubectl create -f manifests/

# It can take a few seconds for the above &#39;create manifests&#39; command to fully create the following resources, so verify the resources are ready before proceeding.
$ until kubectl get customresourcedefinitions servicemonitors.monitoring.coreos.com ; do date; sleep 1; echo &#34;&#34;; done
$ until kubectl get servicemonitors --all-namespaces ; do date; sleep 1; echo &#34;&#34;; done

$ kubectl apply -f manifests/ # This command sometimes may need to be done twice (to workaround a race condition).
</code></pre><p>这里将自动为我们部署prometheus，alertmanager和grafana。我们接下来可以通过port-forward也可以通过ingress将服务暴露出来</p>
<pre tabindex="0"><code>Prometheus

$ kubectl --namespace monitoring port-forward svc/prometheus-k8s 9090

Then access via http://localhost:9090

Grafana

$ kubectl --namespace monitoring port-forward svc/grafana 3000

Then access via http://localhost:3000 and use the default grafana user:password of admin:admin.

Alert Manager

$ kubectl --namespace monitoring port-forward svc/alertmanager-main 9093

Then access via http://localhost:9093
</code></pre><p>或者编写ingress</p>
<pre tabindex="0"><code># ingress-monitor.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: monitoring-ingress
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: &#34;false&#34;
  namespace: monitoring
spec:
  rules:
  - host: k8s-prometheus.calmkart.com
    http:
      paths:
      - path: /
        backend:
          serviceName: prometheus-k8s
          servicePort: 9090
  - host: k8s-grafana.calmkart.com
    http:
      paths:
      - path: /
        backend:
          serviceName: grafana
          servicePort: 3000
  - host: k8s-alertmanager.calmkart.com
    http:
      paths:
      - path: /
        backend:
          serviceName: alertmanager-main
          servicePort: 9093

# kubectl apply -f ingress-monitor.yaml
</code></pre><p>然后我们就可以访问到prometheus,alertmanager和grafana的服务页面了 <a class="link" href="http://www.calmkart.com/wp-content/uploads/2019/08/prometheus.jpg"  target="_blank" rel="noopener"
    ><img src="images/prometheus.jpg" alt="" />
</a></p>
<p><a class="link" href="http://www.calmkart.com/wp-content/uploads/2019/08/alert-manager.jpg"  target="_blank" rel="noopener"
    ><img src="images/alert-manager.jpg" alt="" />
</a></p>
<p><a class="link" href="http://www.calmkart.com/wp-content/uploads/2019/08/grafana.jpg"  target="_blank" rel="noopener"
    ><img src="images/grafana.jpg" alt="" />
</a></p>
<p>这里prometheus已经集成了一些k8s相关的exporter和kubernetes-mixin的报警规则，我们可以从prometheus的status-&gt;rules和status-&gt;target中查看到。</p>
<p>接下来，我们部署<a class="link" href="https://github.com/helm/charts/tree/master/stable/prometheus-pushgateway"  target="_blank" rel="noopener"
    >push-gateway</a></p>
<pre tabindex="0"><code>#可以参考我这里NodePort的values参数,也可以自行设置
# values.yaml

# Default values for prometheus-pushgateway.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
image:
  repository: prom/pushgateway
  tag: v0.9.0
  pullPolicy: IfNotPresent

service:
  type: NodePort
  port: 9091
  targetPort: 9091

# Optional pod annotations
podAnnotations: {}

# Optional pod labels
podLabels: {}

# Optional service labels
serviceLabels: {}

# Optional serviceAccount labels
serviceAccountLabels: {}

# Optional additional arguments
extraArgs: []

# Optional additional environment variables
extraVars: []

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after &#39;resources:&#39;.
  # limits:
  #   cpu: 200m
  #    memory: 50Mi
  # requests:
  #   cpu: 100m
  #   memory: 30Mi

serviceAccount:
  # Specifies whether a ServiceAccount should be created
  create: true
  # The name of the ServiceAccount to use.
  # If not set and create is true, a name is generated using the fullname template
  name:

## Configure ingress resource that allow you to access the
## pushgateway installation. Set up the URL
## ref: http://kubernetes.io/docs/user-guide/ingress/
##
ingress:
  ## Enable Ingress.
  ##
  enabled: false

    ## Annotations.
    ##
    # annotations:
    #   kubernetes.io/ingress.class: nginx
    #   kubernetes.io/tls-acme: &#39;true&#39;

    ## Hostnames.
    ## Must be provided if Ingress is enabled.
    ##
    # hosts:
    #   - pushgateway.domain.com

    ## TLS configuration.
    ## Secrets must be manually created in the namespace.
    ##
    # tls:
    #   - secretName: pushgateway-tls
    #     hosts:
    #       - pushgateway.domain.com

tolerations: {}
  # - effect: NoSchedule
  #   operator: Exists

## Node labels for pushgateway pod assignment
## Ref: https://kubernetes.io/docs/user-guide/node-selection/
##
nodeSelector: {}

replicaCount: 1

## Affinity for pod assignment
## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
affinity: {}

# Enable this if you&#39;re using https://github.com/coreos/prometheus-operator
serviceMonitor:
  enabled: true
  namespace: monitoring
  # fallback to the prometheus default unless specified
  # interval: 10s
  ## Defaults to what&#39;s used if you follow CoreOS [Prometheus Install Instructions](https://github.com/helm/charts/tree/master/stable/prometheus-operator#tldr)
  ## [Prometheus Selector Label](https://github.com/helm/charts/tree/master/stable/prometheus-operator#prometheus-operator-1)
  ## [Kube Prometheus Selector Label](https://github.com/helm/charts/tree/master/stable/prometheus-operator#exporters)
  selector:
    prometheus: kube-prometheus
  # Retain the job and instance labels of the metrics pushed to the Pushgateway
  # [Scraping Pushgateway](https://github.com/prometheus/pushgateway#configure-the-pushgateway-as-a-target-to-scrape)
  honorLabels: true

# The values to set in the PodDisruptionBudget spec (minAvailable/maxUnavailable)
# If not set then a PodDisruptionBudget will not be created
podDisruptionBudget:

# helm install --name push-gateway -f values.yaml stable/prometheus-pushgateway
</code></pre><p>然后我们来试着接入内部和外部的prometheus监控target.</p>
<p>1.实现接入内部的target 无论是外部或内部的target都需要一个metrics-server目标，对于内部target而言，一般是一个服务，比如服务calm-server 在prometheus-operater的使用方式中，有一个crd叫serviceMonitor，我们创建一个新的serviceMonitor就创建了一个prometheus的target 我们首先查看monitoring命名空间中已有的serviceMonitor(既prometheus target)</p>
<pre tabindex="0"><code>[xxx@xxxxx]# kubectl get servicemonitors.monitoring.coreos.com -n monitoring
NAME                      AGE
alertmanager              23d
coredns                   23d
grafana                   23d
ingress-nginx             17d
kube-apiserver            23d
kube-controller-manager   23d
kube-scheduler            23d
kube-state-metrics        23d
kubelet                   23d
node-exporter             23d
prometheus                23d
prometheus-operator       23d
prometheus-pushgateway    19d
</code></pre><p>我们创建一个新的serviceMonitor,将calm-server的/metrics作为target # calm-server-serviceMonitor.yaml</p>
<pre tabindex="0"><code>apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: calm-server-metrics
  labels:
    k8s-app: calm-server-metrics
  namespace: monitoring
spec:
  namespaceSelector:
    any: true
  selector:
    matchLabels:
      app: calm-server
  endpoints:
  - port: web
    interval: 10s
    honorLabels: true

# kubectl apply -f calm-server-serviceMonitor.yaml
</code></pre><p>这里有几个需要注意的tips:</p>
<p>1.如果service和prometheus不再同一个命名空间，需要设置namespaceSelector，可以单独设置需要搜索的namespace，也可以像上面那样设置为全局搜索。</p>
<p>2.其次，endpoints中设置的port会按照interval设置的时间定时去:/metrics拉取数据，所以对应的服务需要提供相应的metrics(官方有非常多的exporter提供使用，可以参考<a class="link" href="https://prometheus.io/docs/instrumenting/exporters/"  target="_blank" rel="noopener"
    >https://prometheus.io/docs/instrumenting/exporters/</a>)</p>
<p>我们接下来再查看所有serviceMonitor crd api对象，就会发现创建成功，同时prometheus的state-&gt;target中也会出现对应的target，我们也可以在prometheus中查询到对应的数据了。</p>
<p>2.实现接入外部target 实现了接入内部的target，那么，k8s集群外部的服务想要接入该怎么办呢？当然还是通过监控k8s集群内service的方式，不是service对应的是一个外部的endpoint对象。下面我们将以calm-server服务为例，说明如何通过k8s的endpoint外部对象接入外部target监控。 首先我们创建一个endpoint对象</p>
<pre tabindex="0"><code># calm-server-endpoint.yaml
apiVersion: v1
kind: Endpoints
metadata:
  name: calm-server-metrics
subsets:
  - addresses:
    - ip: x.x.x.x
    ports:
    - name: metrics
      port: xxxx
      protocol: TCP
# kubectl apply -f calm-server-endpoint.yaml
</code></pre><p>我们将ip和port替换成我们的外部服务，apply后就创建了一个endpoint api对象，我们可以通过kubectl get endpoints查看</p>
<pre tabindex="0"><code>[xxx@xxxxxxx]# kubectl get endpoints
NAME                                  ENDPOINTS                                                           AGE
calm-server-metrics                   10.41.13.17:6789                                                    19d
kubernetes                            10.1.33.159:6443                                                    92d
push-gateway-prometheus-pushgateway   10.240.224.15:9091                                                  19d
</code></pre><p>这里需要注意的是,endpoint对象是不区分namespaces的 接着，我们创建一个service，service的选择器选择calm-server-metrics这个外部endpoint</p>
<pre tabindex="0"><code># calm-server-metrics-service.yaml

apiVersion: v1
kind: Service
metadata:
  name: calm-server-metrics
  labels:
    app: calm-server-metrics
spec:
  type: ExternalName
  externalName: x.x.x.x
  clusterIP: &#34;&#34;
  ports:
  - name: metrics
    port: xxxx
    protocol: TCP
    targetPort: 6789

# kubectl apply -f calm-server-metrics-service.yaml
</code></pre><p>我们这个时候访问这个服务，就等于访问了外部的endpoint，既外部服务 我们就可以像上面创建内部服务的serviceMonitor一样，创建serviceMonitor api对象从而更新prometheus target列表了。</p>
<p>接下来我们会有一个问题，如何修改这一系列全家桶的配置呢?比如prometheus的配置，grafana的配置。比如我们需要修改smtp报警配置该怎么办呢？如果是非云原生环境，我们可以直接修改配置文件即可，但在云原生环境中不一样。</p>
<p>kube-prometheus官方文档推荐的方式是使用<a class="link" href="https://jsonnet.org/"  target="_blank" rel="noopener"
    >jsonnet</a>对官方库做编译修改。那么如何直接通过修改yaml的方式修改配置文件呢？下面将分别介绍如何修改全家桶中的各个配置文件。</p>
<p>1.首先是prometheus的alert rules，我们可以通过修改prometheus-rules.yaml文件修改。 2.其次是alertmanager的config,我们可以修改alertmanager-secret.yaml文件,注意，这是一个secret对象，内容经过了base64加密，我们应该先将内容解密再做修改，修改后再加密替换即可。 3.最后是grafana的配置修改，参考了grafana官方的docker image之后，我们可以先修改grafana-deployment.yaml文件，为其增加一个volume,配置如下</p>
<pre tabindex="0"><code>apiVersion: apps/v1beta2
kind: Deployment
metadata:
  labels:
    app: grafana
  name: grafana
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
    spec:
      containers:
      - image: grafana/grafana:6.2.2
        name: grafana
        ports:
        - containerPort: 3000
          name: http
        readinessProbe:
          httpGet:
            path: /api/health
            port: http
        resources:
          limits:
            cpu: 200m
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 100Mi
        volumeMounts:
        - mountPath: /var/lib/grafana
          name: grafana-storage
          readOnly: false
        - mountPath: /etc/grafana/provisioning/datasources
          name: grafana-datasources
          readOnly: false
        - mountPath: /etc/grafana/provisioning/dashboards
          name: grafana-dashboards
          readOnly: false
        - mountPath: /grafana-dashboard-definitions/0/apiserver
          name: grafana-dashboard-apiserver
          readOnly: false
        - mountPath: /grafana-dashboard-definitions/0/controller-manager
          name: grafana-dashboard-controller-manager
          readOnly: false
        - mountPath: /grafana-dashboard-definitions/0/k8s-cluster-rsrc-use
          name: grafana-dashboard-k8s-cluster-rsrc-use
          readOnly: false
        - mountPath: /grafana-dashboard-definitions/0/k8s-node-rsrc-use
          name: grafana-dashboard-k8s-node-rsrc-use
          readOnly: false
        - mountPath: /grafana-dashboard-definitions/0/k8s-resources-cluster
          name: grafana-dashboard-k8s-resources-cluster
          readOnly: false
        - mountPath: /grafana-dashboard-definitions/0/k8s-resources-namespace
          name: grafana-dashboard-k8s-resources-namespace
          readOnly: false
        - mountPath: /grafana-dashboard-definitions/0/k8s-resources-pod
          name: grafana-dashboard-k8s-resources-pod
          readOnly: false
        - mountPath: /grafana-dashboard-definitions/0/k8s-resources-workload
          name: grafana-dashboard-k8s-resources-workload
          readOnly: false
        - mountPath: /grafana-dashboard-definitions/0/k8s-resources-workloads-namespace
          name: grafana-dashboard-k8s-resources-workloads-namespace
          readOnly: false
        - mountPath: /grafana-dashboard-definitions/0/kubelet
          name: grafana-dashboard-kubelet
          readOnly: false
        - mountPath: /grafana-dashboard-definitions/0/nodes
          name: grafana-dashboard-nodes
          readOnly: false
        - mountPath: /grafana-dashboard-definitions/0/persistentvolumesusage
          name: grafana-dashboard-persistentvolumesusage
          readOnly: false
        - mountPath: /grafana-dashboard-definitions/0/pods
          name: grafana-dashboard-pods
          readOnly: false
        - mountPath: /grafana-dashboard-definitions/0/prometheus-remote-write
          name: grafana-dashboard-prometheus-remote-write
          readOnly: false
        - mountPath: /grafana-dashboard-definitions/0/prometheus
          name: grafana-dashboard-prometheus
          readOnly: false
        - mountPath: /grafana-dashboard-definitions/0/proxy
          name: grafana-dashboard-proxy
          readOnly: false
        - mountPath: /grafana-dashboard-definitions/0/scheduler
          name: grafana-dashboard-scheduler
          readOnly: false
        - mountPath: /grafana-dashboard-definitions/0/statefulset
          name: grafana-dashboard-statefulset
          readOnly: false
        - mountPath: /etc/grafana
          name: config-custom
      nodeSelector:
        beta.kubernetes.io/os: linux
      securityContext:
        runAsNonRoot: true
        runAsUser: 65534
      serviceAccountName: grafana
      volumes:
      - emptyDir: {}
        name: grafana-storage
      - name: grafana-datasources
        secret:
          secretName: grafana-datasources
      - configMap:
          name: grafana-dashboards
        name: grafana-dashboards
      - configMap:
          name: grafana-dashboard-apiserver
        name: grafana-dashboard-apiserver
      - configMap:
          name: grafana-dashboard-controller-manager
        name: grafana-dashboard-controller-manager
      - configMap:
          name: grafana-dashboard-k8s-cluster-rsrc-use
        name: grafana-dashboard-k8s-cluster-rsrc-use
      - configMap:
          name: grafana-dashboard-k8s-node-rsrc-use
        name: grafana-dashboard-k8s-node-rsrc-use
      - configMap:
          name: grafana-dashboard-k8s-resources-cluster
        name: grafana-dashboard-k8s-resources-cluster
      - configMap:
          name: grafana-dashboard-k8s-resources-namespace
        name: grafana-dashboard-k8s-resources-namespace
      - configMap:
          name: grafana-dashboard-k8s-resources-pod
        name: grafana-dashboard-k8s-resources-pod
      - configMap:
          name: grafana-dashboard-k8s-resources-workload
        name: grafana-dashboard-k8s-resources-workload
      - configMap:
          name: grafana-dashboard-k8s-resources-workloads-namespace
        name: grafana-dashboard-k8s-resources-workloads-namespace
      - configMap:
          name: grafana-dashboard-kubelet
        name: grafana-dashboard-kubelet
      - configMap:
          name: grafana-dashboard-nodes
        name: grafana-dashboard-nodes
      - configMap:
          name: grafana-dashboard-persistentvolumesusage
        name: grafana-dashboard-persistentvolumesusage
      - configMap:
          name: grafana-dashboard-pods
        name: grafana-dashboard-pods
      - configMap:
          name: grafana-dashboard-prometheus-remote-write
        name: grafana-dashboard-prometheus-remote-write
      - configMap:
          name: grafana-dashboard-prometheus
        name: grafana-dashboard-prometheus
      - configMap:
          name: grafana-dashboard-proxy
        name: grafana-dashboard-proxy
      - configMap:
          name: grafana-dashboard-scheduler
        name: grafana-dashboard-scheduler
      - configMap:
          name: grafana-dashboard-statefulset
        name: grafana-dashboard-statefulset
      - configMap:
          name: grafana-config
        name: config-custom
</code></pre><p>其实就是将一个叫grafana-config的configMap作为volumeMount到/etc/grafana下。 然后我们创建这个configMap</p>
<pre tabindex="0"><code># grafana-configmap.yaml

apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-config
  namespace: monitoring
data:
  grafana.ini: |
    [smtp]
    enabled = true
    host = smtp.calmkart.com:465
    user = calmkart@calmkart.com
    password = xxxxxxxxxx
    skip_verify = false
    from_address = calmkart@calmkart.com
    from_name = grafana

# kubectl apply -f grafana-configmap.yaml
</code></pre><p>这样就可以替换掉默认的配置文件了。</p>
<p>另外还有关于如何为grafana增加plugin等等话题，可以参考官方的相关资料。 就简单介绍到这里吧。</p>
</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/tags/alertmanager/">Alertmanager</a>
        
            <a href="/tags/grafana/">Grafana</a>
        
            <a href="/tags/k8s/">K8s</a>
        
            <a href="/tags/kubernetes/">Kubernetes</a>
        
            <a href="/tags/prometheus/">Prometheus</a>
        
            <a href="/tags/%E4%BA%91%E5%8E%9F%E7%94%9F/">云原生</a>
        
            <a href="/tags/%E5%AE%B9%E5%99%A8/">容器</a>
        
            <a href="/tags/%E7%9B%91%E6%8E%A7/">监控</a>
        
            <a href="/tags/%E8%BF%90%E7%BB%B4/">运维</a>
        
    </section>


    </footer>


    
</article>

    

    

<aside class="related-content--wrapper">
    <h2 class="section-title">Related content</h2>
    <div class="related-content">
        <div class="flex article-list--tile">
            
                
<article class="">
    <a href="/posts/2019/08/2019-08-14-grafana-dashboard-django-bug%E4%BF%AE%E5%A4%8D/">
        

        <div class="article-details">
            <h2 class="article-title">grafana dashboard django bug修复</h2>
        </div>
    </a>
</article>
            
                
<article class="">
    <a href="/posts/2019/06/2019-06-13-kubernetesjenkins%E5%AE%9E%E7%8E%B0%E7%8E%B0%E4%BB%A3cicd%E6%B5%81%E6%B0%B4%E7%BA%BF%E4%B8%80/">
        

        <div class="article-details">
            <h2 class="article-title">kubernetes&#43;jenkins实现现代cicd流水线(一)</h2>
        </div>
    </a>
</article>
            
                
<article class="">
    <a href="/posts/2019/05/2019-05-25-kubernetes%E9%9B%86%E7%BE%A4%E4%B8%AD%E6%9C%8D%E5%8A%A1%E7%9A%84%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%92%8C%E5%A4%96%E9%83%A8%E5%8F%91%E7%8E%B0/">
        

        <div class="article-details">
            <h2 class="article-title">kubernetes集群中服务的负载均衡和外部发现</h2>
        </div>
    </a>
</article>
            
                
<article class="">
    <a href="/posts/2019/05/2019-05-24-kubernetes%E6%90%AD%E5%BB%BA%E5%B9%B6%E4%BD%BF%E7%94%A8rook-ceph%E7%9A%84pv%E5%AD%98%E5%82%A8/">
        

        <div class="article-details">
            <h2 class="article-title">kubernetes搭建并使用rook/ceph的pv存储</h2>
        </div>
    </a>
</article>
            
                
<article class="">
    <a href="/posts/2019/08/2019-08-14-helm-chart%E7%9A%84%E5%88%B6%E4%BD%9C%E5%8F%8A%E4%BD%BF%E7%94%A8%E9%A1%BA%E4%BE%BF%E6%8B%89%E4%B8%AA%E7%A5%A8/">
        

        <div class="article-details">
            <h2 class="article-title">helm chart的制作及使用(顺便拉个票)</h2>
        </div>
    </a>
</article>
            
        </div>
    </div>
</aside>

     
    
        
        <script src="//cdn.jsdelivr.net/npm/twikoo@1.6.44/dist/twikoo.all.min.js"></script>
<div id="tcomment"></div>
<style>
    .twikoo {
        background-color: var(--card-background);
        border-radius: var(--card-border-radius);
        box-shadow: var(--shadow-l1);
        padding: var(--card-padding);
    }
    :root[data-scheme="dark"] {
        --twikoo-body-text-color-main: rgba(255, 255, 255, 0.9);
        --twikoo-body-text-color: rgba(255, 255, 255, 0.7);
    }
    .twikoo .el-input-group__prepend,
    .twikoo .tk-action-icon,
    .twikoo .tk-submit-action-icon,
    .twikoo .tk-time,
    .twikoo .tk-comments-no,
    .twikoo .tk-comments-count {
        color: var(--twikoo-body-text-color);
    }
    .twikoo .el-input__inner,
    .twikoo .el-textarea__inner,
    .twikoo .tk-preview-container,
    .twikoo .tk-content,
    .twikoo .tk-nick,
    .twikoo .tk-send,
    .twikoo .tk-comments-no,
    .twikoo .el-input__count,
    .twikoo .tk-submit-action-icon {
        color: var(--twikoo-body-text-color-main)!important;
    }
    .twikoo .el-button{
        color: var(--twikoo-body-text-color)!important;
    }
    .twikoo .el-input__count {
        color: var(--twikoo-body-text-color) !important;
    }
    .OwO .OwO-body {
        background-color: var(--body-background) !important;
        color: var(--body-text-color) !important;
    }
</style><script>
    twikoo.init({
        envId: '',
        el: '#tcomment',})
</script>
    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
        2026 cAlm的个人Blog
    </section>
    
    <section class="powerby">
        Built with <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> <br />
        Theme <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="4.0.0">Stack</a></b> designed by <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a>
    </section>
</footer>


    



<script type="module">
    import gallery from '\/ts\/gallery.js';

    const articleContent = document.querySelector('.article-content');
    const shouldLoad = articleContent && (articleContent.querySelectorAll('figure').length > 0 || articleContent.querySelectorAll('img.gallery-image').length > 0);
    
    if (shouldLoad) {
        gallery(articleContent);

        const PhotoSwipeLightbox = (await import("https:\/\/cdn.jsdelivr.net\/npm\/photoswipe@5.4.4\/dist\/photoswipe-lightbox.esm.min.js")).default;
        const styleHref = "https:\/\/cdn.jsdelivr.net\/npm\/photoswipe@5.4.4\/dist\/photoswipe.css";

        const styleTag = document.createElement('link');
        styleTag.rel = 'stylesheet';
        styleTag.href = styleHref;
        document.head.appendChild(styleTag);

        const lightbox = new PhotoSwipeLightbox({
            gallerySelector: '.article-content',
            childSelector: 'figure a.image-link',
            pswpModule: () => import("https:\/\/cdn.jsdelivr.net\/npm\/photoswipe@5.4.4\/dist\/photoswipe.esm.min.js")
        });
        lightbox.init();
    }
</script>
    

            </main>
        </div>
        
<script type="text/javascript" src="/ts/main.2d3f4e9dd7b2453cd71ff8971f18724ef8b4f783f2e8c70b9a281707db74e8a5.js" defer></script>


    </body>
</html>
